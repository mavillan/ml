{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Proyecto de Máquinas de Aprendizaje - ILI393\n",
    "## _Identificación facial cuando existen pocas muestras por clase_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El Problema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El problema consiste en la correcta identificación y reconocimiento facial, dado el caso particular en donde existen muy pocas muestras por clase (por persona) para entrenar los algoritmos algoritmos de clasificación. La mayoría de los enfoques usados en este tipo de problemas consisten en encontrar una buena representación de las características importantes de las caras, para posteriormente realizar algún tipo de búsqueda (jerárquica, nearest neighbors, etc). Sin embargo se propone aquí resolver el problema con tres algoritmos de clasificación distintos: Linear Discriminant Analysis, Support Vector Machines (con kernel lineal y radio basal) y por Convolutional Neural Networks.\n",
    "\n",
    "Los dataset a ocupar son [Faces94](http://cswww.essex.ac.uk/mv/allfaces/faces94.html), [Faces95](http://cswww.essex.ac.uk/mv/allfaces/faces95.html) y [Faces96](http://cswww.essex.ac.uk/mv/allfaces/faces96.html). Cada uno de los datasets consiste en 20 imágenes de individuos, y variable cantidad de individuos. Los datasets están ordenados en cuanto a su complejidad de reconocimiento de menor a mayor. Se muestran a continuación imágenes representativas de Faces94, Faces95 y Faces96 respectivamente.\n",
    "\n",
    "<img src=\"faces94ex.jpg\">\n",
    "\n",
    "<img src=\"faces95ex.jpg\">\n",
    "\n",
    "<img src=\"faces96ex.jpg\">\n",
    "\n",
    "Para cada uno de estos datasets, se crearon los 20 training y testing sets correspondientes. La metodología fue la siguiente: Para cada dataset (94,95,96) se tomaron aleatoriamente entre 2-5 fotos por clase (por persona) para formar los training sets correspondientes, y las restantes 15-18 fotos se dejaron para crear los testing sets correspondientes. Vale decir, si se entrena con train94/5pc (faces94, 5 samples per class) entonces se prueba con test94/15pc (faces94, 15 samples per class). Cumpliendo de este modo con la restricción de tener pocos samples para entrenar los algoritmos.\n",
    "**Observación**: Por el momento se trabaja sólo con Faces94 y Faces95.\n",
    "\n",
    "Para la comparación entre los resultados de los distintos algoritmos, se realizan *Error Bars* del error rate sobre los 20 datasets. Para computar las barras de error, se computan según el intervalo de confianza del 95% para la media del error, por medio de la distribución t-student."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enfoque 1: Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La implementación ocupada corresponde la de Scikit-Learn. Para cada una de las clases (personas) genera la función discriminante lineal $\\delta_k$, que permite diferenciar a cada una de las clases. Los dos supuestos fuertes que se realizan sobre los datasets al ocupar este método, son que 1) La probabilidad multivariada de las características $P(x_m | y=k)$ se distribuye normal, y que 2) La matriz de covarianza para cada una de las clases es igual.\n",
    "\n",
    "Debido a que LDA es un modelo generativo sin _hiperparámetros_, es que no es necesario realizar cross-validation para el modelo. Esto es una gran ventaja, pues ahora gran tiempo de computación (comparado con los otros métodos). Sin embargo, como se verá más adelante, paga este costo de simplicidad, entregando resultados menos precisos y generalizantes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) LDA con Faces94"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lda_tr_err_faces94_3spc, lda_ts_err_faces94_3spc = solve_lda('faces94', 3, verbose=False)\n",
    "lda_tr_err_faces94_4spc, lda_ts_err_faces94_4spc = solve_lda('faces94', 4, verbose=False)\n",
    "lda_tr_err_faces94_5spc, lda_ts_err_faces94_5spc = solve_lda('faces94', 5, verbose=False)\n",
    "#storing results\n",
    "np.save('lda_tr_err_faces94_3spc',lda_tr_err_faces94_3spc); np.save('lda_ts_err_faces94_3spc',lda_ts_err_faces94_3spc)\n",
    "np.save('lda_tr_err_faces94_4spc',lda_tr_err_faces94_4spc); np.save('lda_ts_err_faces94_4spc',lda_ts_err_faces94_4spc)\n",
    "np.save('lda_tr_err_faces94_5spc',lda_tr_err_faces94_5spc); np.save('lda_ts_err_faces94_5spc',lda_ts_err_faces94_5spc)\n",
    "tr_errors_lda_faces94 = [lda_tr_err_faces94_3spc, lda_tr_err_faces94_4spc, lda_tr_err_faces94_5spc]\n",
    "ts_errors_lda_faces94 = [lda_ts_err_faces94_3spc, lda_ts_err_faces94_4spc, lda_ts_err_faces94_5spc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAuIAAAGJCAYAAADPOFY7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f804c0583d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_errorbars(ts_errors_lda_faces94, title='some title')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Análisis:**\n",
    "+ De los gráficos, se nota que el dataset Faces94 es muy \"fácil\" y por lo tanto los resultados son casi perfectos. Si se revisan las imagenes en este, puede notarse que las imagenes para una misma persona son todas muy parecidas, es decir, hay poca variación en la pose, expresión, iluminación, etc. Por lo cual esta data es fácilmente separable por hiperplanos.\n",
    "+ De todos modos, a medida que aumentan las muestras por clase, el error rate tiende a disminuir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA con Faces95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martin/anaconda2/lib/python2.7/site-packages/sklearn/discriminant_analysis.py:453: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "lda_tr_err_faces95_3spc, lda_ts_err_faces95_3spc = solve_lda('faces95', 3, verbose=False)\n",
    "lda_tr_err_faces95_4spc, lda_ts_err_faces95_4spc = solve_lda('faces95', 4, verbose=False)\n",
    "lda_tr_err_faces95_5spc, lda_ts_err_faces95_5spc = solve_lda('faces95', 5, verbose=False)\n",
    "#storing results\n",
    "np.save('lda_tr_err_faces95_3spc',lda_tr_err_faces95_3spc); np.save('lda_ts_err_faces95_3spc',lda_ts_err_faces95_3spc)\n",
    "np.save('lda_tr_err_faces95_4spc',lda_tr_err_faces95_4spc); np.save('lda_ts_err_faces95_4spc',lda_ts_err_faces95_4spc)\n",
    "np.save('lda_tr_err_faces95_5spc',lda_tr_err_faces95_5spc); np.save('lda_ts_err_faces95_5spc',lda_ts_err_faces95_5spc)\n",
    "tr_errors_lda_faces95 = [lda_tr_err_faces95_3spc, lda_tr_err_faces95_4spc, lda_tr_err_faces95_5spc]\n",
    "ts_errors_lda_faces95 = [lda_ts_err_faces95_3spc, lda_ts_err_faces95_4spc, lda_ts_err_faces95_5spc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtsAAAGJCAYAAABb3v/JAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f804c0581d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_errorbars(tr_errors_lda_faces95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Análisis:**\n",
    "+ Se nota claramente que el dataset Faces95 es un dataset mucho más complejo que el anterior.\n",
    "+ Los error rates son claramente mayores, siendo estos muy altos para los casos en donde hay pocos ejemplos por clase.\n",
    "+ De todas maneras el comportamiento tiende a mejorar cuando existen más muestras por clase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enfoque 2: Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Marco Teórico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machines (SVM) es un método de clasificación binario (adaptable a problemas de múltiples clases), que encuentra la frontera de decisión lineal óptima (hiperplano óptimo) que separa a las clases. Intuituvamente, una buena separación es lograda por el hiperplano con mayor distancia o margen a los datos de entramiento más cercanos. Esta superficie de separación es una combinación lineal de elementos del training set, conocidos como vectores de soporte, pues definen la frontera entre dos clases\n",
    "<img src=\"separating_hyperplane.png\" style=\"width: 500px;\">\n",
    "\n",
    "Por lo tanto el método se reduce a el problema de optimización, de encontrar el hiperplano $\\mathbf{w}^T x + b = 0$ que maximice el margen de cada ejemplo en el training set. Esto puede expresarse mas formalmente como:\n",
    "\\begin{align}\n",
    "& \\min_{\\mathbf{w},b} \\frac{1}{2} ||\\mathbf{w}||^2 \\\\\n",
    "& \\text{s.t } \\ y_m(\\mathbf{w}^T x_m + b) \\geq 1, \\ m=1,\\ldots ,M \n",
    "\\end{align}\n",
    "\n",
    "lo cual corresponde a un problema de optimización cuadrático. Es posible relajar algunas reestricciones del problema introduciendo *variables de holgura*\n",
    "\\begin{align}\n",
    "& \\min_ {\\mathbf{w}, b, \\zeta} \\frac{1}{2} ||\\mathbf{w}||^2 + C \\sum_{m=1}^{M} \\zeta_i \\\\\n",
    "& \\text{s.t } \\ y_m (\\mathbf{w}^T \\ x_m + b) \\geq 1 - \\zeta_m,\\\\\n",
    "& \\zeta_m \\geq 0, m=1, ..., M\n",
    "\\end{align}\n",
    "\n",
    "donde el parámetro C controla el trade-off entre el error de clasificación y el tamaño del margen. En general a mayor margen, menor será el error de generalización del clasificador, por lo tanto con las *variables de holgura* se tiene una manera de controlar esta relación.\n",
    "\n",
    "La SVM determinada por el anterior problema de optimización es conocida como la C-SVM. Sin embargo por medio de una re-parametrización de esta, es posible definir un nuevo parámetros $\\nu$ que controla el número de vectores de soporte y el error de entrenamiento. Esta formulación corresponde a la $\\nu$-SVM y puede ser demostrado que ambas formulaciones son matemáticamente equivalentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimentación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El enfoque seguido en esta sección, es la implementación y entrenamiento de multiclass SVM's con kernels tanto lineales como gaussianos, para cumplir con el objetivo de clasificar la data. Para ello se ocupan $\\nu$-SMV's, debido a que facilita la configuración del parámetro de holgura $\\nu$. La implementación ocupada corresponde a la de Scikit-Learn, el cual advierte: _“SVC  implement one-against-one” approach (Knerr et al., 1990) for multi- class classification.\"_\n",
    "\n",
    "Para la selección de _hiperparámetros_ $\\nu$ y $\\gamma$ (en kernels rbf) se realiza *stratified cross-validation* sobre cada training set, con la ayuda de grid search. Para que este método tenga sentido, el número de folds debe ser igual al número de muestras por clase (si hay más folds que muestras por clase, no se pueden cumplir las condiciones de estratificación).\n",
    "\n",
    "Debido a que las dimensiones de las imágenes (200x180=36000) corresponden al total de features de cada foto, se ha decidido realizar una reducción de dimensionalidad para mejorar los tiempos de entrenamientos y eficiencia, así como también tomar las características realmente importantes (aquellas que permiten diferencias entre las clases).\n",
    "\n",
    "Como técnica de reducción de dimensionalidad, se ha decidido ocupar LDA como reducción de dimensionalidad supervisada, representación tambien conocida como [_Fisher Faces_](http://www.scholarpedia.org/article/Fisherfaces). Esta técnica\n",
    "intenta encontrar un subespacio donde proyectar la data que permita diferenciar de mejor manera las clases. Dicho de otro modo, se intenta maximizar la **inter-class variance**  y minimizar la varianza **intra-class variance**.\n",
    "\n",
    "Básicamente los hiperplanos que conforman el subespacio de representación, corresponden a la funciones discriminantes que genera el modelo en LDA. La idea se plasma en la siguiente representación\n",
    "\n",
    "<img src=\"lineardisc.jpg\">\n",
    "\n",
    "en donde cada dato se proyecta en los hiperplanos discriminantes, para formar la representación. El número de máximo de discriminantes es $\\min(\\text{dimensiones},\\text{clases}-1)$, por lo tanto en un problema con muchas más dimensiones (features) que clases, la reducción de dimensionalidad es considerablemente favorable (este problema es precisamente el caso). Se pueden ocupar menos discriminantes, pero para los experimentos que se muestran a continuación se ocupan todos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Linear SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observaciones:**\n",
    "+ Los rangos de los parámetros fueron determinados empíricamente, probando aquellos que entregan resultados coherentes y útiles en los datasets respectivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Setting parameters to try on Linear-SVM for Faces94 and Faces95\n",
    "Nu = np.linspace(0.0005, 0.001, 5, endpoint=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear-SVM con Faces94"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lsvm_tr_err_faces94_3spc, lsvm_ts_err_faces94_3spc = solve_svm('faces94', 3, 'linear', Nu, verbose=False)\n",
    "lsvm_tr_err_faces94_4spc, lsvm_ts_err_faces94_4spc = solve_svm('faces94', 4, 'linear', Nu, verbose=False)\n",
    "lsvm_tr_err_faces94_5spc, lsvm_ts_err_faces94_5spc = solve_svm('faces94', 5, 'linear', Nu, verbose=False)\n",
    "#storing results\n",
    "np.save('lsvm_tr_err_faces94_3spc',lsvm_tr_err_faces94_3spc); np.save('lsvm_ts_err_faces94_3spc',lsvm_ts_err_faces94_3spc)\n",
    "np.save('lsvm_tr_err_faces94_4spc',lsvm_tr_err_faces94_4spc); np.save('lsvm_ts_err_faces94_4spc',lsvm_ts_err_faces94_4spc)\n",
    "np.save('lsvm_tr_err_faces94_5spc',lsvm_tr_err_faces94_5spc); np.save('lsvm_ts_err_faces94_5spc',lsvm_ts_err_faces94_5spc)\n",
    "tr_errors_lsvm_faces94 = [lsvm_tr_err_faces94_3spc, lsvm_tr_err_faces94_4spc, lsvm_tr_err_faces94_5spc]\n",
    "ts_errors_lsvm_faces94 = [lsvm_ts_err_faces94_3spc, lsvm_ts_err_faces94_4spc, lsvm_ts_err_faces94_5spc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_errorbars(ts_errors_lsvm_faces94)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El gráfico a continuación compara los resultados recien obtenidos, con los resultados de los algoritmos anteriores para el dataset Faces94"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"comp1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Análisis:**\n",
    "+ Sucede lo mismo que con LDA: Faces94 es muy fácil de separar por hiperplanos, y por lo tanto obtiene un buen resultado también con la SVM de kernel lineal.\n",
    "+ Los error rates obtenidos aquí son ligeramente inferiores a los obtenidos con LDA.\n",
    "+ Sigue el patrón de mejorar los resultados a medida que aumentan las muestras por clase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear-SVM con Faces95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lsvm_tr_err_faces95_3spc, lsvm_ts_err_faces95_3spc = solve_svm('faces95', 3, 'linear', Nu, verbose=False)\n",
    "lsvm_tr_err_faces95_4spc, lsvm_ts_err_faces95_4spc = solve_svm('faces95', 4, 'linear', Nu, verbose=False)\n",
    "lsvm_tr_err_faces95_5spc, lsvm_ts_err_faces95_5spc = solve_svm('faces95', 5, 'linear', Nu, verbose=False)\n",
    "#storing results\n",
    "np.save('lsvm_tr_err_faces95_3spc',lsvm_tr_err_faces95_3spc); np.save('lsvm_ts_err_faces95_3spc',lsvm_ts_err_faces95_3spc)\n",
    "np.save('lsvm_tr_err_faces95_4spc',lsvm_tr_err_faces95_4spc); np.save('lsvm_ts_err_faces95_4spc',lsvm_ts_err_faces95_4spc)\n",
    "np.save('lsvm_tr_err_faces95_5spc',lsvm_tr_err_faces95_5spc); np.save('lsvm_ts_err_faces95_5spc',lsvm_ts_err_faces95_5spc)\n",
    "tr_errors_lsvm_faces95 = [lsvm_tr_err_faces95_3spc, lsvm_tr_err_faces95_4spc, lsvm_tr_err_faces95_5spc]\n",
    "ts_errors_lsvm_faces95 = [lsvm_ts_err_faces95_3spc, lsvm_ts_err_faces95_4spc, lsvm_ts_err_faces95_5spc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_errorbars(ts_errors_lsvm_faces95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El gráfico a continuación compara los resultados recien obtenidos, con los resultados de los algoritmos anteriores para el dataset Faces95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"comp2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Análisis:**\n",
    "+ Los resultados aquí obtenidos, mejoran en una cantidad notoria respecto a el mismo dataset aplicado con LDA. Sin embargo, no hay que olvidar que aquí se tuvo que realizar un proceso de cross-validation para establecer _hiperparámetros_ y en LDA no.\n",
    "+ La mejor capacidad de generalización obtenida por la SVM, refleja los beneficios de tener variables de holgura, esto es, permitir cometer ciertos errores en el training set, para asi aumentar el margen junto con la capacidad de generalizar en el testing set.\n",
    "+ Se mantiene el patrón de mejorar los resultados a medida que aumentan los ejemplos por clase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Kernel-SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observaciones:**\n",
    "+ Los rangos de los parámetros fueron determinados empíricamente, probando aquellos que entregan resultados coherentes y útiles en los datasets respectivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#setting parameters to try one kernel-svm\n",
    "Nu = np.linspace(0.0005, 0.01, 10, endpoint=True)\n",
    "Gamma = np.linspace(0.25, 2.0, 10, endpoint=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Kernel-SVM con Faces94 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ksvm_tr_err_faces94_3spc, ksvm_ts_err_faces94_3spc = solve_svm('faces94', 3, 'rbf', Nu, Gamma, verbose=False)\n",
    "ksvm_tr_err_faces94_4spc, ksvm_ts_err_faces94_4spc = solve_svm('faces94', 4, 'rbf', Nu, Gamma, verbose=False)\n",
    "ksvm_tr_err_faces94_5spc, ksvm_ts_err_faces94_5spc = solve_svm('faces94', 5, 'rbf', Nu, Gamma, verbose=False)\n",
    "#storing results\n",
    "np.save('ksvm_tr_err_faces94_3spc',ksvm_tr_err_faces94_3spc); np.save('ksvm_ts_err_faces94_3spc',ksvm_ts_err_faces94_3spc)\n",
    "np.save('ksvm_tr_err_faces94_4spc',ksvm_tr_err_faces94_4spc); np.save('ksvm_ts_err_faces94_4spc',ksvm_ts_err_faces94_4spc)\n",
    "np.save('ksvm_tr_err_faces94_5spc',ksvm_tr_err_faces94_5spc); np.save('ksvm_ts_err_faces94_5spc',ksvm_ts_err_faces94_5spc)\n",
    "tr_errors_ksvm_faces94 = [ksvm_tr_err_faces94_3spc, ksvm_tr_err_faces94_4spc, ksvm_tr_err_faces94_5spc]\n",
    "ts_errors_ksvm_faces94 = [ksvm_ts_err_faces94_3spc, ksvm_ts_err_faces94_4spc, ksvm_ts_err_faces94_5spc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_errorbars(ts_errors_ksvm_faces94)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El gráfico a continuación compara los resultados recien obtenidos, con los resultados de los algoritmos anteriores para el dataset Faces94"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"comp3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Análisis:**\n",
    "+ Sorprendentemente, los resultados son signigicativamente peores que con los otros dos algoritmos anteriores. Peor aun, el error en vez de disminiur a medida que aumentan las muestras por clase, aumenta.\n",
    "+ La explicación más probable de este fenómeno, es overfitting. Como sabemos, con un kernel gaussiano se da la posibilidad de aprender fronteras de decisión no lineales complejas. Sin embargo sabemos que este dataset es muy bien separable por hiperplanos, por lo tanto el hecho de agregarle complejidad, sólo empeoró la situación.\n",
    "+ Adicionalmente el hecho de que el error aumente con las muestras por clase, es un reflejo de lo descrito anteriormente. Mientras más muestras por clase existe, va a generar un modelo no lineal más y más complejo, lo cual para efectos de este dataset, es algo malo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel-SVM con Faces95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#setting parameters to try one kernel-svm\n",
    "Nu = np.linspace(0.0005, 0.01, 10, endpoint=True)\n",
    "Gamma = np.linspace(0.25, 2.0, 10, endpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ksvm_tr_err_faces95_3spc, ksvm_ts_err_faces95_3spc = solve_svm('faces95', 3, 'rbf', Nu, Gamma, verbose=False)\n",
    "ksvm_tr_err_faces95_4spc, ksvm_ts_err_faces95_4spc = solve_svm('faces95', 4, 'rbf', Nu, Gamma, verbose=False)\n",
    "ksvm_tr_err_faces95_5spc, ksvm_ts_err_faces95_5spc = solve_svm('faces95', 5, 'rbf', Nu, Gamma, verbose=False)\n",
    "#storing results\n",
    "np.save('ksvm_tr_err_faces95_3spc',ksvm_tr_err_faces95_3spc); np.save('ksvm_ts_err_faces95_3spc',ksvm_ts_err_faces95_3spc)\n",
    "np.save('ksvm_tr_err_faces95_4spc',ksvm_tr_err_faces95_4spc); np.save('ksvm_ts_err_faces95_4spc',ksvm_ts_err_faces95_4spc)\n",
    "np.save('ksvm_tr_err_faces95_5spc',ksvm_tr_err_faces95_5spc); np.save('ksvm_ts_err_faces95_5spc',ksvm_ts_err_faces95_5spc)\n",
    "tr_errors_ksvm_faces95 = [ksvm_tr_err_faces95_3spc, ksvm_tr_err_faces95_4spc, ksvm_tr_err_faces95_5spc]\n",
    "ts_errors_ksvm_faces95 = [ksvm_ts_err_faces95_3spc, ksvm_ts_err_faces95_4spc, ksvm_ts_err_faces95_5spc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_errorbars(ts_errors_ksvm_faces95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El gráfico a continuación compara los resultados recien obtenidos, con los resultados de los algoritmos anteriores para el dataset Faces95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"comp4.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Análisis:**\n",
    "+ Los resultados no son mejores que los obtenidos con los algoritmos anteriores, pero sin embargo no son tan malos como resulto en Faces94. Esto pues Faces95 es sabido un dataset más complejo, y por lo tanto un modelo de mayor complejidad (que uno lineal) puede ayudar. \n",
    "+ Los resultados podrían mejorar aún más si se realiza las selección de _hiperparámetros_ en una malla más fina, pero esto requeriría de mucho tiempo de computación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enfoque 3: Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loading the results for faces94\n",
    "cnn_tr_err_faces94_3spc = np.load('./cnn_results/tr_err_faces94_3spc_cnn.npy')\n",
    "cnn_ts_err_faces94_3spc = np.load('./cnn_results/ts_err_faces94_3spc_cnn.npy')\n",
    "cnn_tr_err_faces94_4spc = np.load('./cnn_results/tr_err_faces94_4spc_cnn.npy')\n",
    "cnn_ts_err_faces94_4spc = np.load('./cnn_results/ts_err_faces94_4spc_cnn.npy')\n",
    "cnn_tr_err_faces94_5spc = np.load('./cnn_results/tr_err_faces94_5spc_cnn.npy')\n",
    "cnn_ts_err_faces94_5spc = np.load('./cnn_results/ts_err_faces94_5spc_cnn.npy')\n",
    "\n",
    "#loading results for faces95\n",
    "cnn_tr_err_faces95_3spc = np.load('./cnn_results/tr_err_faces95_3spc_cnn.npy')\n",
    "cnn_ts_err_faces95_3spc = np.load('./cnn_results/ts_err_faces95_3spc_cnn.npy')\n",
    "cnn_tr_err_faces95_4spc = np.load('./cnn_results/tr_err_faces95_4spc_cnn.npy')\n",
    "cnn_ts_err_faces95_4spc = np.load('./cnn_results/ts_err_faces95_4spc_cnn.npy')\n",
    "cnn_tr_err_faces95_5spc = np.load('./cnn_results/tr_err_faces95_5spc_cnn.npy')\n",
    "cnn_ts_err_faces95_5spc = np.load('./cnn_results/ts_err_faces95_5spc_cnn.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enfoque 4: Dissimilarity SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Marco Teórico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Representación\n",
    "\n",
    "El problema de *identificación* tratado corresponde a dada una imagen de prueba $\\mathbf{p}$, determinar a qué clase del conjunto de entrenamiento $S_{tr}$ corresponde. Los enfoques basado en SVM anteriores, reducen este problema a $K$-class classification.\n",
    "\n",
    "El propósito del método planteado, es ocupar las capacidades de clasificación de las SVM como clasificador binario, sobre dos conjuntos $C_1$ y $C_2$, donde el primero corresponde al *within-class differences set* que contienen las disimilitudes entre datos de la misma clase, y el segundo es *between-class difference set* y contienen las disimilitudes entre datos de distinta clase. El espacio en el que habitan los elementos de estos conjuntos, es conocido como el *difference space*, y contrasta con el espacio standard de las imágenes conocido como *image space*.\n",
    "\n",
    "Formalizando lo anterior; sea $S_{tr} = \\{\\mathbf{s}_1 , \\ldots, \\mathbf{s}_M \\}$ el conjunto de entrenamiento con imágenes faciales de $K$ individuos. Para indicar que dos individuos pertenecen a la misma clase ocuparemos $\\mathbf{s}_i \\sim \\mathbf{s}_j$, y en caso contrario $\\mathbf{s}_i \\nsim \\mathbf{s}_j$. Se define adicionalmente la función de similitud $\\phi : R^N \\times R^N \\rightarrow R^S$ con $S \\leq N$, como aquella función que mapea dos images, hacia el *difference space*. Luego es posible definir\n",
    "\\begin{align}\n",
    "& C_1 = \\{\\phi(\\mathbf{s}_i, \\mathbf{s}_j)\\  | \\ \\mathbf{s}_i \\sim \\mathbf{s}_j \\} \\\\\n",
    "& C_2 = \\{\\phi(\\mathbf{s}_i, \\mathbf{s}_j)\\  | \\ \\mathbf{s}_i \\nsim \\mathbf{s}_j \\}\n",
    "\\end{align}\n",
    "\n",
    "#### Entrenamiento\n",
    "\n",
    "Para el entrenamiento de la D-SVM (Dissimilarity SVM) los conjuntos de entrada son $C_1$ y $C_2$, es decir, se realiza un simple entrenamiento para clasificación binaria, lo cual es bastante conveniente.\n",
    "\n",
    "Adicionalmente, sabemos que el output de una SVM es un conjunto de $M_s$ vectores de soporte $\\mathbf{v}_m$, coeficientes $\\alpha_m$, etiquetas de las clases $y_m$ de los vectores de soporte y el término $b$ constante. Luego la superficie de decisión puede ser escrita como\n",
    "$$\n",
    "f(\\mathbf{x}) = \\sum_m^{M_s} \\alpha_m y_m K(\\mathbf{v}_m, \\mathbf{x}) + b = 0\n",
    "$$\n",
    "\n",
    "donde $K(\\cdot,\\cdot)$ es una función de kernel de acuerdo al Mercer's Theorem. Para $f(\\mathbf{x}_i)<0$, mientras más grande sea el valor de $|f(\\mathbf{x_i})|$, más es el grado (o probabilidad) de pertenencia de $\\mathbf{x}_i$ a la primera clase (de modo análogo se concluye para la segunda clase). Por lo tanto se puede ocupar la función $f$ como discriminante, o como score de pertenencia a una clase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dsvm_tr_err_faces94_3spc, dsvm_ts_err_faces94_3spc = solve_dsvm('faces94', 3, verbose=False)\n",
    "dsvm_tr_err_faces94_4spc, dsvm_ts_err_faces94_4spc = solve_dsvm('faces94', 4, verbose=False)\n",
    "dsvm_tr_err_faces94_5spc, dsvm_ts_err_faces94_5spc = solve_dsvm('faces94', 5, verbose=False)\n",
    "#storing results\n",
    "np.save('dsvm_tr_err_faces94_3spc',dsvm_tr_err_faces94_3spc); np.save('dsvm_ts_err_faces94_3spc',dsvm_ts_err_faces94_3spc)\n",
    "np.save('dsvm_tr_err_faces94_4spc',dsvm_tr_err_faces94_4spc); np.save('dsvm_ts_err_faces94_4spc',dsvm_ts_err_faces94_4spc)\n",
    "np.save('dsvm_tr_err_faces94_5spc',dsvm_tr_err_faces94_5spc); np.save('dsvm_ts_err_faces94_5spc',dsvm_ts_err_faces94_5spc)\n",
    "tr_errors_dsvm_faces94 = [dsvm_tr_err_faces94_3spc, dsvm_tr_err_faces94_4spc, dsvm_tr_err_faces94_5spc]\n",
    "ts_errors_dsvm_faces94 = [dsvm_ts_err_faces94_3spc, dsvm_ts_err_faces94_4spc, dsvm_ts_err_faces94_5spc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dsvm_tr_err_faces95_3spc, dsvm_ts_err_faces95_3spc = solve_dsvm('faces95', 3, verbose=False)\n",
    "dsvm_tr_err_faces95_4spc, dsvm_ts_err_faces95_4spc = solve_dsvm('faces95', 4, verbose=False)\n",
    "dsvm_tr_err_faces95_5spc, dsvm_ts_err_faces95_5spc = solve_dsvm('faces95', 5, verbose=False)\n",
    "#storing results\n",
    "np.save('dsvm_tr_err_faces95_3spc',dsvm_tr_err_faces95_3spc); np.save('dsvm_ts_err_faces95_3spc',dsvm_ts_err_faces95_3spc)\n",
    "np.save('dsvm_tr_err_faces95_4spc',dsvm_tr_err_faces95_4spc); np.save('dsvm_ts_err_faces95_4spc',dsvm_ts_err_faces95_4spc)\n",
    "np.save('dsvm_tr_err_faces95_5spc',dsvm_tr_err_faces95_5spc); np.save('dsvm_ts_err_faces95_5spc',dsvm_ts_err_faces95_5spc)\n",
    "tr_errors_dsvm_faces95 = [dsvm_tr_err_faces95_3spc, dsvm_tr_err_faces95_4spc, dsvm_tr_err_faces95_5spc]\n",
    "ts_errors_dsvm_faces95 = [dsvm_ts_err_faces95_3spc, dsvm_ts_err_faces95_4spc, dsvm_ts_err_faces95_5spc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_errorbars(ts_errors_dsvm_faces94)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_errorbars(ts_errors_dsvm_faces95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anexo de Código"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuración del notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cv2 as cv\n",
    "from sklearn import svm, grid_search\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "\n",
    "#libraries for proposed approach\n",
    "import histogram #own library\n",
    "from sklearn.metrics.pairwise import chi2_kernel\n",
    "from skimage.feature import local_binary_pattern as lbp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "> function to load data from path directory to a matrix.\n",
    "> each row of the resulting matrix, corresponds to a flattened image\n",
    "  in grayscale format\n",
    "\"\"\"\n",
    "def load_data(path, stacked=False):\n",
    "    #total number of classes\n",
    "    M = len(os.listdir(path))\n",
    "    #dimensions of each image\n",
    "    N = 200*180\n",
    "    #samples per class\n",
    "    spc = int(path.strip().split('-')[1][:-2])\n",
    "    #matrix with features\n",
    "    if stacked:\n",
    "        data = np.empty((M*spc,200,180), dtype=np.float32)\n",
    "    else: \n",
    "        data = np.empty((M*spc,N), dtype=np.float32)\n",
    "    labels = np.empty(M*spc, dtype=np.uint8)\n",
    "    #index of data matrix\n",
    "    m = 0\n",
    "    for i in xrange(1,M+1):\n",
    "        tgt = path+str(i)+'/'\n",
    "        pics = os.listdir(tgt)\n",
    "        for pic in pics:\n",
    "            if stacked:\n",
    "                #store each image, as a bidimensional array in data matrix\n",
    "                data[m,:,:] = cv.imread(tgt+pic, cv.IMREAD_GRAYSCALE)\n",
    "            else:\n",
    "                #store each flattened image, as a row in data matrix\n",
    "                data[m,:] = cv.imread(tgt+pic, cv.IMREAD_GRAYSCALE).ravel()\n",
    "            labels[m] = i\n",
    "            m += 1\n",
    "    return (data, labels)\n",
    "\n",
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.figure(figsize=(15,8))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "    \n",
    "def make_hist(winners,params):\n",
    "    for key in winners:\n",
    "        par = params[key]\n",
    "        win = np.array(winners[key])\n",
    "        freqs = np.zeros(5)\n",
    "        for i in xrange(5):\n",
    "            freqs[i] = np.sum(par[i]==win)\n",
    "        labels = map(str,par)\n",
    "        pos = np.arange(len(labels))\n",
    "        width = 1.0\n",
    "        fig = plt.figure()\n",
    "        fig.set_figheight(6)\n",
    "        fig.set_figwidth(6)\n",
    "        ax = plt.axes()\n",
    "        ax.set_xticks(pos + (width / 2))\n",
    "        ax.set_xticklabels(labels)\n",
    "        plt.xlabel('Parameters')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title('Histogram of {0}'.format(key))\n",
    "        plt.bar(pos, freqs, width, color='0.5')\n",
    "        plt.show()\n",
    "\n",
    "def mean_confidence_interval(data, confidence=0.9):\n",
    "    #computing confidence interval\n",
    "    n = data.shape[0]\n",
    "    mu = np.mean(data)\n",
    "    sem = sp.stats.sem(data)\n",
    "    h1,h2 = sp.stats.t.interval(confidence, n-1, loc=mu, scale=sem)\n",
    "    return mu, h1, h2\n",
    "\n",
    "def mean_percentile(data, inf=10, sup=90):\n",
    "    mu = np.mean(data)\n",
    "    h1, h2 = np.percentile(data, [10,90])\n",
    "    return mu, h1, h2\n",
    "\n",
    "def plot_errorbars(errors, title=None):\n",
    "    mean = []\n",
    "    lower = []\n",
    "    upper = []\n",
    "    for error in errors:\n",
    "        mu,l,u = mean_confidence_interval(error)\n",
    "        mean.append(mu)\n",
    "        lower.append(l)\n",
    "        upper.append(u)\n",
    "    x = np.arange(3, len(errors)+3)\n",
    "    mean = np.array(mean)\n",
    "    lower = mean-np.array(lower)\n",
    "    upper = np.array(upper)-mean\n",
    "    labels = ['3spc','4spc','5spc']\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.xlim([2,6])\n",
    "    plt.xticks(x, labels)\n",
    "    plt.errorbar(x, mean, yerr=[lower,upper],fmt='o')\n",
    "    plt.xlabel('Samples per class')\n",
    "    plt.ylabel('Mean error rate')\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funciones de error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def precision(y_ts, y_pd):\n",
    "    #true positives\n",
    "    tp = (y_ts==y_pd).sum()\n",
    "    #total of predictions\n",
    "    n = y_ts.shape[0]\n",
    "    return tp/np.float(n)\n",
    "\n",
    "def error_rate(y_ts, y_pd):\n",
    "    return 1-precision(y_ts, y_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funciones para LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def solve_lda(dataset, spc, verbose=False):\n",
    "    #samples per class on training set\n",
    "    spc_tr = spc\n",
    "    spc_ts = 20-spc_tr\n",
    "    #training and testing paths\n",
    "    tr_path = './db/train'+dataset[-2:]+'/tr-{0}pc-{1}/'\n",
    "    ts_path = './db/test'+dataset[-2:]+'/ts-{0}pc-{1}/'\n",
    "    #errors through all datasets\n",
    "    tr_err = list()\n",
    "    ts_err = list()\n",
    "    #iterating through datasets\n",
    "    for set_num in xrange(20):\n",
    "        #loading training and testing set\n",
    "        X_tr,y_tr = load_data(tr_path.format(spc_tr,set_num))\n",
    "        X_ts,y_ts = load_data(ts_path.format(spc_ts,set_num))\n",
    "        #creating LDA object and fitting the testing data\n",
    "        clf = LDA()\n",
    "        clf.fit(X_tr, y_tr)\n",
    "        #computing training error\n",
    "        tr_err.append(1.-clf.score(X_tr,y_tr))\n",
    "        #computing testing error\n",
    "        ts_err.append(1.-clf.score(X_ts,y_ts))\n",
    "        if verbose:\n",
    "            print \"#####################################################################################\"\n",
    "            print \"{0}: {1} samples per class (dataset {2})\".format(dataset, spc, set_num)\n",
    "            print \"Training error rate: {0}\".format(tr_err[-1])\n",
    "            print \"Testing error rate: {0}\".format(ts_err[-1])\n",
    "        #releasing memory of big objects\n",
    "        del X_tr, X_ts, clf\n",
    "    return np.array(tr_err),np.array(ts_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funciones para SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Supervised dimensionality reduction through LDA\n",
    "\"\"\"\n",
    "def fisher_faces(X, y):\n",
    "    #supervised learning through LDA\n",
    "    #finding the discriminant functions\n",
    "    ff = LDA()\n",
    "    ff.fit(X,y)\n",
    "    #proyect the data into linear discriminant hyperplanes\n",
    "    return ff\n",
    "\n",
    "\"\"\"\n",
    "Stratified 5-fold cross validation y Grid search\n",
    "para determinar el mejor parametro C en linear svm\n",
    "\"\"\"\n",
    "def cross_linear_svm(X, y, Nu, n_folds=None):\n",
    "    #generating stratified 5-fold cross validation iterator\n",
    "    strat_kf = StratifiedKFold(y, n_folds=n_folds, shuffle=True)\n",
    "    #parameters to try\n",
    "    params = {'nu':Nu}\n",
    "    #Setting grid search for linear-svm\n",
    "    clf = svm.NuSVC(kernel='linear')\n",
    "    gs = grid_search.GridSearchCV(clf, params, cv=strat_kf, n_jobs=2)\n",
    "    #make it\n",
    "    gs.fit(X, y)\n",
    "    #return best parameters and grid scores\n",
    "    return gs.best_params_['nu'] , gs.grid_scores_\n",
    "\n",
    "\"\"\"\n",
    "Stratified 5-fold cross validation y Grid search\n",
    "para determinar el mejor parametro Nu y Gamma en rbf svm\n",
    "\"\"\"\n",
    "def cross_rbf_svm(X, y, Nu, Gamma, n_folds=None):\n",
    "    #generating stratified 5-fold cross validation iterator\n",
    "    strat_kf = StratifiedKFold(y, n_folds=n_folds, shuffle=True)\n",
    "    #parameters to try\n",
    "    params = {'nu':Nu, 'gamma':Gamma}\n",
    "    #Setting grid search for rbf-svm\n",
    "    clf = svm.NuSVC(kernel='rbf')\n",
    "    gs = grid_search.GridSearchCV(clf, params, cv=strat_kf, n_jobs=2)\n",
    "    #make it\n",
    "    gs.fit(X, y)\n",
    "    #return best parameters and grid scores\n",
    "    return gs.best_params_['nu'], gs.best_params_['gamma'] , gs.grid_scores_\n",
    "\n",
    "\n",
    "def solve_svm(dataset, spc, kernel, Nu, Gamma=None, verbose=False):\n",
    "    #samples per class on training set\n",
    "    spc_tr = spc\n",
    "    spc_ts = 20-spc_tr\n",
    "    #training and testing paths\n",
    "    tr_path = './db/train'+dataset[-2:]+'/tr-{0}pc-{1}/'\n",
    "    ts_path = './db/test'+dataset[-2:]+'/ts-{0}pc-{1}/'\n",
    "    #errors through all datasets\n",
    "    tr_err = list()\n",
    "    ts_err = list()\n",
    "    #iterating through datasets\n",
    "    for set_num in xrange(20):\n",
    "        #loading training and testing sets\n",
    "        X_tr,y_tr = load_data(tr_path.format(spc_tr,set_num))\n",
    "        X_ts,y_ts = load_data(ts_path.format(spc_ts,set_num))\n",
    "        #projecting into discriminant space\n",
    "        ff = fisher_faces(X_tr, y_tr)\n",
    "        X_tr = ff.transform(X_tr)\n",
    "        X_ts = ff.transform(X_ts)\n",
    "        #choosing best nu (and gamma) through stratified\n",
    "        #5-fold cross-validation and grid search\n",
    "        if kernel=='linear':\n",
    "            nu,grid_scores = cross_linear_svm(X_tr, y_tr, Nu, n_folds=spc)\n",
    "        elif kernel=='rbf':\n",
    "            nu,gamma,grid_scores = cross_rbf_svm(X_tr, y_tr, Nu, Gamma, n_folds=spc)\n",
    "        #fitting the model\n",
    "        if kernel=='linear':\n",
    "            clf = svm.NuSVC(kernel='linear', nu=nu)\n",
    "        elif kernel=='rbf':\n",
    "            clf = svm.NuSVC(kernel='rbf', nu=nu, gamma=gamma)    \n",
    "        clf.fit(X_tr,y_tr)\n",
    "        #computing training error\n",
    "        tr_err.append(1.-clf.score(X_tr,y_tr))\n",
    "        #computing testing error\n",
    "        ts_err.append(1.-clf.score(X_ts,y_ts))\n",
    "        if verbose:\n",
    "            print \"#####################################################################################\"\n",
    "            print \"{0}: {1} samples per class (dataset {2})\".format(dataset, spc, set_num)\n",
    "            print \"Training error rate {0}\".format(tr_err[-1])\n",
    "            print \"Testing error rate: {0}\".format(ts_err[-1])\n",
    "            print \"Best Nu: {0}\".format(nu)\n",
    "            if kernel=='rbf':\n",
    "                print \"Best gamma: {0}\".format(gamma)\n",
    "        #releasing memory of big objects\n",
    "        del X_tr, X_ts, clf\n",
    "    return np.array(tr_err),np.array(ts_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funciones para EigenSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class EigenSVM(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, n_components=100, nu=0.1, gamma=0.5):\n",
    "        self.n_components = n_components\n",
    "        self.nu = nu\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def to_eigenspace(self, X):\n",
    "        #computing PCA\n",
    "        pca = PCA(n_components=self.n_components, copy=True)\n",
    "        pca.fit(X)\n",
    "        #storing pca object\n",
    "        self.pca = pca\n",
    "        return pca.transform(X)\n",
    "    \n",
    "    def build_classes(self, X, y, spc):\n",
    "        #dimensions of input matrix\n",
    "        M,N = X.shape\n",
    "        #indexes of X\n",
    "        indexes = np.arange(M)\n",
    "        np.random.shuffle(indexes)\n",
    "        #dimensions of output matrices\n",
    "        P = int(sp.misc.comb(spc,2))*152\n",
    "        Q = self.n_components\n",
    "        \"\"\"\n",
    "        > building the within class difference set C1\n",
    "        > all combinations between samples of the same\n",
    "          class are computed\n",
    "        \"\"\"\n",
    "        C1 = np.empty((P,Q))\n",
    "        p = 0 #index of C1\n",
    "        for m in xrange(0, M, spc):\n",
    "            for i in xrange(m, m+spc):\n",
    "                for j in xrange(i+1, m+spc):\n",
    "                    #difference space representation\n",
    "                    C1[p] = np.abs(X[i] - X[j])\n",
    "                    p += 1\n",
    "        \"\"\"\n",
    "        > building the between class difference set\n",
    "        > randomly selecting two samples of different classes\n",
    "        \"\"\"\n",
    "        C2 = np.empty((P,Q))\n",
    "        p = 0 #index of C2\n",
    "        while p < P:\n",
    "            i,j = np.random.choice(indexes, 2)\n",
    "            #indexes must be of different classes\n",
    "            if y[i]==y[j]: continue\n",
    "            C2[p] = np.abs(X[i] - X[j])\n",
    "            p += 1\n",
    "        #return both classes\n",
    "        return (C1,C2)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        #counting samples per class\n",
    "        frec = np.bincount(y-1)\n",
    "        spc = frec[0]\n",
    "        #if any class has different frec\n",
    "        if not np.all(spc==frec): return -1\n",
    "        \n",
    "        #to eigenspace\n",
    "        eig_X = self.to_eigenspace(X)\n",
    "        \n",
    "        #building the classes\n",
    "        C1,C2 = self.build_classes(eig_X, y, spc)\n",
    "        data = np.concatenate((C1,C2), axis=0)\n",
    "        #label arrays\n",
    "        lab1 = np.ones(C1.shape[0],dtype=int)\n",
    "        lab2 = 2*lab1\n",
    "        labels = np.concatenate((lab1,lab2))\n",
    "        \n",
    "        #building svm with stratified cross validation\n",
    "        strat_kf = StratifiedKFold(labels, n_folds=5, shuffle=True)\n",
    "        #parameters to try\n",
    "        Nu = np.linspace(0.1, 0.5, 5, endpoint=True)\n",
    "        Gamma = np.linspace(0.25, 1.5, 5, endpoint=True)\n",
    "        params = {'nu':Nu, 'gamma':Gamma}\n",
    "        #grid search\n",
    "        clf = svm.NuSVC(kernel='rbf')\n",
    "        gs = grid_search.GridSearchCV(clf, params, cv=strat_kf, n_jobs=2)\n",
    "        gs.fit(data,labels)\n",
    "        #fitting with best parameters\n",
    "        eigsvm = svm.NuSVC(kernel='rbf', nu=gs.best_params_['nu'], gamma=gs.best_params_['gamma'])\n",
    "        eigsvm.fit(data,labels)\n",
    "        \n",
    "        #storing important things to make predictions\n",
    "        self.spc = spc\n",
    "        self.data = eig_X\n",
    "        self.eigsvm = eigsvm\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        #X data to spatial histogram format\n",
    "        X = self.to_eigenspace(X)\n",
    "        M = X.shape[0]\n",
    "        predictions = np.empty(M, dtype=np.uint8)\n",
    "        for m in xrange(M):\n",
    "            diff = np.abs(X[m]-self.data)\n",
    "            df = self.eigsvm.decision_function(diff)\n",
    "            N = df.shape[0]\n",
    "            min_val = np.inf\n",
    "            min_ind = 0\n",
    "            for i in xrange(0,N,self.spc):\n",
    "                mean_score = np.sum(df[i:i+self.spc])/self.spc\n",
    "                if mean_score < min_val:\n",
    "                    min_val = mean_score\n",
    "                    min_ind = i/self.spc\n",
    "            predictions[m] = min_ind\n",
    "        #shift to match the labels\n",
    "        predictions += 1\n",
    "        return predictions\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        y_pd = self.predict(X)\n",
    "        return precision(y, y_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Stratified 5-fold cross validation y Grid search\n",
    "para determinar el mejor parametro Nu y Gamma en rbf svm\n",
    "\"\"\"\n",
    "def cross_eigsvm(X, y, N, Nu, Gamma, n_folds):\n",
    "    #generating stratified 5-fold cross validation iterator\n",
    "    strat_kf = StratifiedKFold(y, n_folds=n_folds, shuffle=True)\n",
    "    #parameters to try\n",
    "    params = {'N':N, 'nu':Nu, 'gamma':Gamma}\n",
    "    #Setting grid search for rbf-svm\n",
    "    clf = dSVM()\n",
    "    gs = grid_search.GridSearchCV(clf, params, cv=strat_kf, n_jobs=2)\n",
    "    #make it\n",
    "    gs.fit(X, y)\n",
    "    #return best parameters and grid scores\n",
    "    return gs.best_params_['N'], gs.best_params_['nu'], gs.best_params_['gamma'], gs.grid_scores_\n",
    "    \n",
    "\n",
    "def solve_eigsvm(dataset, spc, verbose=False):\n",
    "    #samples per class on training and testing set\n",
    "    spc_tr = spc\n",
    "    spc_ts = 20-spc_tr\n",
    "    #training and testing paths\n",
    "    tr_path = './db/train'+dataset[-2:]+'/tr-{0}pc-{1}/'\n",
    "    ts_path = './db/test'+dataset[-2:]+'/ts-{0}pc-{1}/'\n",
    "    #errors through all datasets\n",
    "    tr_err = list()\n",
    "    ts_err = list()\n",
    "    #iterating through datasets\n",
    "    for set_num in xrange(20):\n",
    "        #loading training and testing sets\n",
    "        X_tr,y_tr = load_data(tr_path.format(spc_tr,set_num))\n",
    "        X_ts,y_ts = load_data(ts_path.format(spc_ts,set_num))\n",
    "        #choosing best nu (and gamma) through stratified\n",
    "        #5-fold cross-validation and grid search\n",
    "        #n,nu,gamma = cross_dsvm(X_tr, y_tr, N, Nu, Gamma, n_folds=spc)\n",
    "        #fitting the model with this parameters\n",
    "        clf = EigenSVM()    \n",
    "        clf.fit(X_tr,y_tr)\n",
    "        #computing training error\n",
    "        tr_err.append(1.-clf.score(X_tr,y_tr))\n",
    "        #computing testing error\n",
    "        ts_err.append(1.-clf.score(X_ts,y_ts))\n",
    "        if verbose:\n",
    "            print \"#####################################################################################\"\n",
    "            print \"{0}: {1} samples per class (dataset {2})\".format(dataset, spc, set_num)\n",
    "            print \"Training error rate: {0}\".format(tr_err[-1])\n",
    "            print \"Testing error rate: {0}\".format(ts_err[-1])\n",
    "        #releasing memory of big objects\n",
    "        del X_tr, X_ts, clf\n",
    "    return np.array(tr_err),np.array(ts_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_tr, y_tr = load_data('./db/train94/tr-3pc-0/')\n",
    "X_ts, y_ts = load_data('./db/test94/ts-17pc-0/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eigsvm = EigenSVM()\n",
    "eigsvm.fit(X_tr, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dsvm = dSVM()\n",
    "dsvm.fit(X_tr, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dsvm.score(X_ts, y_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eigsvm.score(X_ts, y_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funciones para Dissimilarity SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#chi2 distance metric\n",
    "def chi2(h0, h1, tol=1e-10):\n",
    "    if h1.ndim==1:\n",
    "        return np.sum((h0-h1)**2./((h0+h1)+tol))\n",
    "    elif h1.ndim==2:\n",
    "        return np.sum((h0-h1)**2./((h0+h1)+tol),axis=1)\n",
    "\n",
    "class dSVM(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, P=8, R=2, N=8, OverlapX=2, OverlapY=2, nu=0.1, gamma=0.5):\n",
    "        self.P = P #number of neighbors to compute LBP\n",
    "        self.R = R #radius to compute LBP\n",
    "        self.N = N #Number of partitions on x-axis and y-axis (spatial histogram)\n",
    "        self.Npatterns = 59 #number of bins of each histogram (not settable)\n",
    "        self.OverlapX = OverlapX #allowed overlap between regions on x-axis\n",
    "        self.OverlapY = OverlapY #allowed overlap between regions on y-axis\n",
    "        self.nu = nu #nu parameter for Nu-SVM\n",
    "        self.gamma = gamma #gamma parameter for kernel on SVM\n",
    "    \"\"\"\n",
    "    Transform data on X matrix (flattened) to \n",
    "    spatial histograms representation on Y matrix\n",
    "    \"\"\"\n",
    "    def to_spatial_histogram(self, X):\n",
    "        M,N = X.shape\n",
    "        #return matrix\n",
    "        Y = np.empty((M,self.Npatterns*self.N**2))\n",
    "        for m in xrange(M):\n",
    "            img = X[m].reshape((-1,180))\n",
    "            lbp_img = lbp(img, self.P, self.R, method='nri_uniform')\n",
    "            lbp_img = lbp_img.astype(np.uint8)\n",
    "            sp_hist = histogram.spatial(lbp_img, self.N, self.N,\n",
    "                      self.Npatterns, self.OverlapX, self.OverlapY)\n",
    "            Y[m] = sp_hist\n",
    "        return Y\n",
    "\n",
    "    #compute frecuency difference\n",
    "    #def dissimilarity1(h0, h1):\n",
    "    #    return np.abs(h0-h1)\n",
    "\n",
    "    \"\"\"\n",
    "    > computes dissimilarity between h0 and h1\n",
    "    > h1 could be a 2D array. In that case\n",
    "      dissimilarities are computed between h0 and\n",
    "      each row in h1\n",
    "    \"\"\"\n",
    "    def dissimilarity(self, h0, h1):\n",
    "        if h1.ndim==1:\n",
    "            N = h0.shape[0]\n",
    "            #number of histograms (dimensions on difference space)\n",
    "            S = N/59\n",
    "            diff = np.empty(S)\n",
    "            s = 0 #index of diff\n",
    "            for j in xrange(0,N,59):\n",
    "                diff[s] = chi2(h0[j:j+59],h1[j:j+59])\n",
    "                s += 1\n",
    "            return diff\n",
    "        elif h1.ndim==2:\n",
    "            M,N = h1.shape\n",
    "            #number of histograms (dimensions on difference space)\n",
    "            S = N/59\n",
    "            diff = np.empty((M,S))\n",
    "            s = 0 #column index of diff\n",
    "            for j in xrange(0,N,59):\n",
    "                diff[:,s] = chi2(h0[j:j+59],h1[:,j:j+59])\n",
    "                s += 1\n",
    "            return diff\n",
    "\n",
    "    def build_classes(self, hist_matrix, y, spc):\n",
    "        #dimensions of input matrix\n",
    "        M,N = hist_matrix.shape\n",
    "        #indexes of hist_matrix\n",
    "        indexes = np.arange(M)\n",
    "        np.random.shuffle(indexes)\n",
    "        #dimensions of output matrices\n",
    "        P = int(sp.misc.comb(spc,2))*152\n",
    "        Q = self.N**2\n",
    "        \"\"\"\n",
    "        > building the within class difference set C1\n",
    "        > all combinations between samples of the same\n",
    "          class are computed\n",
    "        \"\"\"\n",
    "        C1 = np.empty((P,Q))\n",
    "        p = 0 #index of C1\n",
    "        for m in xrange(0, M, spc):\n",
    "            for i in xrange(m, m+spc):\n",
    "                for j in xrange(i+1, m+spc):\n",
    "                    #difference space representation\n",
    "                    C1[p] = self.dissimilarity(hist_matrix[i], hist_matrix[j])\n",
    "                    p += 1\n",
    "        \"\"\"\n",
    "        > building the between class difference set\n",
    "        > randomly selecting two samples of different classes\n",
    "        \"\"\"\n",
    "        C2 = np.empty((P,Q))\n",
    "        p = 0 #index of C2\n",
    "        while p < P:\n",
    "            i,j = np.random.choice(indexes, 2)\n",
    "            #indexes must be of different classes\n",
    "            if y[i]==y[j]: continue\n",
    "            C2[p] = self.dissimilarity(hist_matrix[i],hist_matrix[j])\n",
    "            p += 1\n",
    "        #return both classes\n",
    "        return (C1,C2)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        #counting samples per class\n",
    "        frec = np.bincount(y-1)\n",
    "        spc = frec[0]\n",
    "        #if any class has different frec\n",
    "        if not np.all(spc==frec): return -1\n",
    "        \n",
    "        #X data to spatial histogram format\n",
    "        hist_matrix = self.to_spatial_histogram(X)\n",
    "        \n",
    "        #building classes\n",
    "        C1,C2 = self.build_classes(hist_matrix,y,spc)\n",
    "        data = np.concatenate((C1,C2), axis=0)\n",
    "        #label arrays\n",
    "        lab1 = np.ones(C1.shape[0],dtype=int)\n",
    "        lab2 = 2*lab1\n",
    "        labels = np.concatenate((lab1,lab2))\n",
    "        \n",
    "        #building svm with stratified cross validation\n",
    "        strat_kf = StratifiedKFold(labels, n_folds=5, shuffle=True)\n",
    "        #parameters to try\n",
    "        Nu = np.linspace(0.1, 0.5, 5, endpoint=True)\n",
    "        Gamma = np.linspace(0.25, 1.5, 5, endpoint=True)\n",
    "        params = {'nu':Nu, 'gamma':Gamma}\n",
    "        #grid search\n",
    "        clf = svm.NuSVC(kernel='rbf')\n",
    "        gs = grid_search.GridSearchCV(clf, params, cv=strat_kf, n_jobs=2)\n",
    "        gs.fit(data,labels)\n",
    "        #fitting with best parameters\n",
    "        dsvm = svm.NuSVC(kernel='rbf', nu=gs.best_params_['nu'], gamma=gs.best_params_['gamma'])\n",
    "        dsvm.fit(data,labels)\n",
    "        \n",
    "        #storing important things to make predictions\n",
    "        self.spc = spc\n",
    "        self.data = hist_matrix\n",
    "        self.dsvm = dsvm\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        #X data to spatial histogram format\n",
    "        X = self.to_spatial_histogram(X)\n",
    "        M = X.shape[0]\n",
    "        predictions = np.empty(M, dtype=np.uint8)\n",
    "        for m in xrange(M):\n",
    "            diff = self.dissimilarity(X[m],self.data)\n",
    "            df = self.dsvm.decision_function(diff)\n",
    "            N = df.shape[0]\n",
    "            min_val = np.inf\n",
    "            min_ind = 0\n",
    "            for i in xrange(0,N,self.spc):\n",
    "                mean_score = np.sum(df[i:i+self.spc])/self.spc\n",
    "                if mean_score < min_val:\n",
    "                    min_val = mean_score\n",
    "                    min_ind = i/self.spc\n",
    "            predictions[m] = min_ind\n",
    "        #shift to match the labels\n",
    "        predictions += 1\n",
    "        return predictions\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        y_pd = self.predict(X)\n",
    "        return precision(y, y_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Stratified 5-fold cross validation y Grid search\n",
    "para determinar el mejor parametro Nu y Gamma en rbf svm\n",
    "\"\"\"\n",
    "def cross_dsvm(X, y, N, Nu, Gamma, n_folds):\n",
    "    #generating stratified 5-fold cross validation iterator\n",
    "    strat_kf = StratifiedKFold(y, n_folds=n_folds, shuffle=True)\n",
    "    #parameters to try\n",
    "    params = {'N':N, 'nu':Nu, 'gamma':Gamma}\n",
    "    #Setting grid search for rbf-svm\n",
    "    clf = dSVM()\n",
    "    gs = grid_search.GridSearchCV(clf, params, cv=strat_kf, n_jobs=2)\n",
    "    #make it\n",
    "    gs.fit(X, y)\n",
    "    #return best parameters and grid scores\n",
    "    return gs.best_params_['N'], gs.best_params_['nu'], gs.best_params_['gamma'], gs.grid_scores_\n",
    "    \n",
    "\n",
    "def solve_dsvm(dataset, spc, verbose=False):\n",
    "    #samples per class on training and testing set\n",
    "    spc_tr = spc\n",
    "    spc_ts = 20-spc_tr\n",
    "    #training and testing paths\n",
    "    tr_path = './db/train'+dataset[-2:]+'/tr-{0}pc-{1}/'\n",
    "    ts_path = './db/test'+dataset[-2:]+'/ts-{0}pc-{1}/'\n",
    "    #errors through all datasets\n",
    "    tr_err = list()\n",
    "    ts_err = list()\n",
    "    #iterating through datasets\n",
    "    for set_num in xrange(20):\n",
    "        #loading training and testing sets\n",
    "        X_tr,y_tr = load_data(tr_path.format(spc_tr,set_num))\n",
    "        X_ts,y_ts = load_data(ts_path.format(spc_ts,set_num))\n",
    "        #choosing best nu (and gamma) through stratified\n",
    "        #5-fold cross-validation and grid search\n",
    "        #n,nu,gamma = cross_dsvm(X_tr, y_tr, N, Nu, Gamma, n_folds=spc)\n",
    "        #fitting the model with this parameters\n",
    "        clf = dSVM()    \n",
    "        clf.fit(X_tr,y_tr)\n",
    "        #computing training error\n",
    "        tr_err.append(1.-clf.score(X_tr,y_tr))\n",
    "        #computing testing error\n",
    "        ts_err.append(1.-clf.score(X_ts,y_ts))\n",
    "        if verbose:\n",
    "            print \"#####################################################################################\"\n",
    "            print \"{0}: {1} samples per class (dataset {2})\".format(dataset, spc, set_num)\n",
    "            print \"Training error rate: {0}\".format(tr_err[-1])\n",
    "            print \"Testing error rate: {0}\".format(ts_err[-1])\n",
    "        #releasing memory of big objects\n",
    "        del X_tr, X_ts, clf\n",
    "    return np.array(tr_err),np.array(ts_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#to do: build a function for this\n",
    "mean1 = []; mean2 = []; mean3 = []\n",
    "lower1 = []; lower2 = []; lower3 = []\n",
    "upper1 = []; upper2 = []; upper3 = []\n",
    "for error in errors_lda_faces95:\n",
    "    mu,l,u = mean_confidence_interval(error)\n",
    "    mean1.append(mu)\n",
    "    lower1.append(l)\n",
    "    upper1.append(u)\n",
    "for error in errors_lsvm_faces95:\n",
    "    mu,l,u = mean_confidence_interval(error)\n",
    "    mean2.append(mu)\n",
    "    lower2.append(l)\n",
    "    upper2.append(u)\n",
    "for error in errors_dsvm_faces95:\n",
    "    mu,l,u = mean_confidence_interval(error)\n",
    "    mean3.append(mu)\n",
    "    lower3.append(l)\n",
    "    upper3.append(u)\n",
    "mean1 = np.array(mean1); mean2 = np.array(mean2); mean3 = np.array(mean3)\n",
    "lower1 = np.array(lower1); lower2 = np.array(lower2); lower3 = np.array(lower3)\n",
    "upper1 = np.array(upper1); upper2 = np.array(upper2); upper3 = np.array(upper3)\n",
    "x1 = np.array([2,5,8])\n",
    "x2 = np.array([3,6,9])\n",
    "x3 = np.array([4,7,10])\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.xlim([1,11])\n",
    "labels = ['3spc','3spc','3spc','4spc','4spc','4spc','5spc','5spc','5spc']\n",
    "plt.errorbar(x1, mean1, yerr=[lower1,upper1], fmt='o', label='LDA')\n",
    "plt.errorbar(x2, mean2, yerr=[lower2,upper2],fmt='o', label='Linear Nu-SVM')\n",
    "plt.errorbar(x3, mean3, yerr=[lower3,upper3],fmt='o', label='Dissimilarity SVM')\n",
    "plt.xticks([2,3,4,5,6,7,8,9,10],labels)\n",
    "plt.xlabel('Samples per class')\n",
    "plt.ylabel('Mean error rate')\n",
    "plt.title('Error bars with different samples per class: Faces95')\n",
    "plt.legend(bbox_to_anchor=(1.25, 1.0))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
