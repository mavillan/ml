{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea N°2 - Máquinas de Aprendizaje - ILI393\n",
    "### Martín Villanueva A."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 1 - Evaluación de Créditos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################################################\n",
      "Dataset: 0\n",
      "Training error: 0.0777777777778\n",
      "Testing error: 0.133333333333\n",
      "##############################################################\n",
      "\n",
      "\n",
      "##############################################################\n",
      "Dataset: 1\n",
      "Training error: 0.0555555555556\n",
      "Testing error: 0.1\n",
      "##############################################################\n",
      "\n",
      "\n",
      "##############################################################\n",
      "Dataset: 2\n",
      "Training error: 0.0222222222222\n",
      "Testing error: 0.133333333333\n",
      "##############################################################\n",
      "\n",
      "\n",
      "##############################################################\n",
      "Dataset: 3\n",
      "Training error: 0.0555555555556\n",
      "Testing error: 0.166666666667\n",
      "##############################################################\n",
      "\n",
      "\n",
      "##############################################################\n",
      "Dataset: 4\n",
      "Training error: 0.0555555555556\n",
      "Testing error: 0.1\n",
      "##############################################################\n",
      "\n",
      "\n",
      "##############################################################\n",
      "Dataset: 5\n",
      "Training error: 0.0666666666667\n",
      "Testing error: 0.0333333333333\n",
      "##############################################################\n",
      "\n",
      "\n",
      "##############################################################\n",
      "Dataset: 6\n",
      "Training error: 0.0333333333333\n",
      "Testing error: 0.1\n",
      "##############################################################\n",
      "\n",
      "\n",
      "##############################################################\n",
      "Dataset: 7\n",
      "Training error: 0.0555555555556\n",
      "Testing error: 0.1\n",
      "##############################################################\n",
      "\n",
      "\n",
      "##############################################################\n",
      "Dataset: 8\n",
      "Training error: 0.0444444444444\n",
      "Testing error: 0.2\n",
      "##############################################################\n",
      "\n",
      "\n",
      "##############################################################\n",
      "Dataset: 9\n",
      "Training error: 0.0444444444444\n",
      "Testing error: 0.1\n",
      "##############################################################\n",
      "\n",
      "\n",
      "##############################################################\n",
      "Dataset: 10\n",
      "Training error: 0.0555555555556\n",
      "Testing error: 0.0333333333333\n",
      "##############################################################\n",
      "\n",
      "\n",
      "##############################################################\n",
      "Dataset: 11\n",
      "Training error: 0.0666666666667\n",
      "Testing error: 0.1\n",
      "##############################################################\n",
      "\n",
      "\n",
      "##############################################################\n",
      "Dataset: 12\n",
      "Training error: 0.0333333333333\n",
      "Testing error: 0.233333333333\n",
      "##############################################################\n",
      "\n",
      "\n",
      "##############################################################\n",
      "Dataset: 13\n",
      "Training error: 0.0333333333333\n",
      "Testing error: 0.233333333333\n",
      "##############################################################\n",
      "\n",
      "\n",
      "##############################################################\n",
      "Dataset: 14\n",
      "Training error: 0.0444444444444\n",
      "Testing error: 0.2\n",
      "##############################################################\n",
      "\n",
      "\n",
      "##############################################################\n",
      "Dataset: 15\n",
      "Training error: 0.0555555555556\n",
      "Testing error: 0.1\n",
      "##############################################################\n",
      "\n",
      "\n",
      "##############################################################\n",
      "Dataset: 16\n",
      "Training error: 0.0444444444444\n",
      "Testing error: 0.2\n",
      "##############################################################\n",
      "\n",
      "\n",
      "##############################################################\n",
      "Dataset: 17\n",
      "Training error: 0.0444444444444\n",
      "Testing error: 0.2\n",
      "##############################################################\n",
      "\n",
      "\n",
      "##############################################################\n",
      "Dataset: 18\n",
      "Training error: 0.0666666666667\n",
      "Testing error: 0.0333333333333\n",
      "##############################################################\n",
      "\n",
      "\n",
      "##############################################################\n",
      "Dataset: 19\n",
      "Training error: 0.0444444444444\n",
      "Testing error: 0.1\n",
      "##############################################################\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "solve_clf('LDA', 'credit', data_func=normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primal Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#parameters to try on primal perceptron\n",
    "params = {'eta':np.linspace(1e0, 1e2, 5, endpoint=True)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################################################\n",
      "Dataset: 0\n",
      "Best parameter: {'eta': 1.0}\n",
      "CV scores:\n",
      "[mean: 0.91111, std: 0.05554, params: {'eta': 1.0}, mean: 0.91111, std: 0.05554, params: {'eta': 25.75}, mean: 0.91111, std: 0.05554, params: {'eta': 50.5}, mean: 0.91111, std: 0.05554, params: {'eta': 75.25}, mean: 0.91111, std: 0.05554, params: {'eta': 100.0}]\n",
      "Training error: 0.0\n",
      "Testing error: 0.1\n",
      "##############################################################\n",
      "\n",
      "\n",
      "##############################################################\n",
      "Dataset: 1\n",
      "Best parameter: {'eta': 1.0}\n",
      "CV scores:\n",
      "[mean: 0.92222, std: 0.07565, params: {'eta': 1.0}, mean: 0.92222, std: 0.07565, params: {'eta': 25.75}, mean: 0.92222, std: 0.07565, params: {'eta': 50.5}, mean: 0.92222, std: 0.07565, params: {'eta': 75.25}, mean: 0.92222, std: 0.07565, params: {'eta': 100.0}]\n",
      "Training error: 0.0\n",
      "Testing error: 0.0666666666667\n",
      "##############################################################\n",
      "\n",
      "\n",
      "##############################################################\n",
      "Dataset: 2\n",
      "Best parameter: {'eta': 1.0}\n",
      "CV scores:\n",
      "[mean: 0.95556, std: 0.05303, params: {'eta': 1.0}, mean: 0.95556, std: 0.05303, params: {'eta': 25.75}, mean: 0.95556, std: 0.05303, params: {'eta': 50.5}, mean: 0.95556, std: 0.05303, params: {'eta': 75.25}, mean: 0.95556, std: 0.05303, params: {'eta': 100.0}]\n",
      "Training error: 0.0\n",
      "Testing error: 0.133333333333\n",
      "##############################################################\n",
      "\n",
      "\n",
      "##############################################################\n",
      "Dataset: 3\n",
      "Best parameter: {'eta': 1.0}\n",
      "CV scores:\n",
      "[mean: 0.91111, std: 0.07098, params: {'eta': 1.0}, mean: 0.91111, std: 0.07098, params: {'eta': 25.75}, mean: 0.91111, std: 0.07098, params: {'eta': 50.5}, mean: 0.91111, std: 0.07098, params: {'eta': 75.25}, mean: 0.91111, std: 0.07098, params: {'eta': 100.0}]\n",
      "Training error: 0.0222222222222\n",
      "Testing error: 0.0333333333333\n",
      "##############################################################\n",
      "\n",
      "\n",
      "##############################################################\n",
      "Dataset: 4\n",
      "Best parameter: {'eta': 1.0}\n",
      "CV scores:\n",
      "[mean: 0.95556, std: 0.04157, params: {'eta': 1.0}, mean: 0.95556, std: 0.04157, params: {'eta': 25.75}, mean: 0.95556, std: 0.04157, params: {'eta': 50.5}, mean: 0.95556, std: 0.04157, params: {'eta': 75.25}, mean: 0.95556, std: 0.04157, params: {'eta': 100.0}]\n",
      "Training error: 0.0333333333333\n",
      "Testing error: 0.1\n",
      "##############################################################\n",
      "\n",
      "\n",
      "##############################################################\n",
      "Dataset: 5\n",
      "Best parameter: {'eta': 1.0}\n",
      "CV scores:\n",
      "[mean: 0.87778, std: 0.10049, params: {'eta': 1.0}, mean: 0.87778, std: 0.10049, params: {'eta': 25.75}, mean: 0.87778, std: 0.10049, params: {'eta': 50.5}, mean: 0.87778, std: 0.10049, params: {'eta': 75.25}, mean: 0.87778, std: 0.10049, params: {'eta': 100.0}]\n",
      "Training error: 0.0333333333333\n",
      "Testing error: 0.1\n",
      "##############################################################\n",
      "\n",
      "\n",
      "##############################################################\n",
      "Dataset: 6\n",
      "Best parameter: {'eta': 1.0}\n",
      "CV scores:\n",
      "[mean: 0.93333, std: 0.05115, params: {'eta': 1.0}, mean: 0.93333, std: 0.05115, params: {'eta': 25.75}, mean: 0.93333, std: 0.05115, params: {'eta': 50.5}, mean: 0.93333, std: 0.05115, params: {'eta': 75.25}, mean: 0.93333, std: 0.05115, params: {'eta': 100.0}]\n",
      "Training error: 0.0333333333333\n",
      "Testing error: 0.0333333333333\n",
      "##############################################################\n",
      "\n",
      "\n",
      "##############################################################\n",
      "Dataset: 7\n",
      "Best parameter: {'eta': 1.0}\n",
      "CV scores:\n",
      "[mean: 0.90000, std: 0.03836, params: {'eta': 1.0}, mean: 0.90000, std: 0.03836, params: {'eta': 25.75}, mean: 0.90000, std: 0.03836, params: {'eta': 50.5}, mean: 0.90000, std: 0.03836, params: {'eta': 75.25}, mean: 0.90000, std: 0.03836, params: {'eta': 100.0}]\n",
      "Training error: 0.0222222222222\n",
      "Testing error: 0.1\n",
      "##############################################################\n",
      "\n",
      "\n",
      "##############################################################\n",
      "Dataset: 8\n",
      "Best parameter: {'eta': 1.0}\n",
      "CV scores:\n",
      "[mean: 0.98889, std: 0.02353, params: {'eta': 1.0}, mean: 0.98889, std: 0.02353, params: {'eta': 25.75}, mean: 0.98889, std: 0.02353, params: {'eta': 50.5}, mean: 0.98889, std: 0.02353, params: {'eta': 75.25}, mean: 0.98889, std: 0.02353, params: {'eta': 100.0}]\n",
      "Training error: 0.0\n",
      "Testing error: 0.0666666666667\n",
      "##############################################################\n",
      "\n",
      "\n",
      "##############################################################\n",
      "Dataset: 9\n",
      "Best parameter: {'eta': 1.0}\n",
      "CV scores:\n",
      "[mean: 0.94444, std: 0.03724, params: {'eta': 1.0}, mean: 0.94444, std: 0.03724, params: {'eta': 25.75}, mean: 0.94444, std: 0.03724, params: {'eta': 50.5}, mean: 0.94444, std: 0.03724, params: {'eta': 75.25}, mean: 0.94444, std: 0.03724, params: {'eta': 100.0}]\n",
      "Training error: 0.0\n",
      "Testing error: 0.2\n",
      "##############################################################\n",
      "\n",
      "\n",
      "##############################################################\n",
      "Dataset: 10\n",
      "Best parameter: {'eta': 1.0}\n",
      "CV scores:\n",
      "[mean: 0.95556, std: 0.03995, params: {'eta': 1.0}, mean: 0.95556, std: 0.03995, params: {'eta': 25.75}, mean: 0.95556, std: 0.03995, params: {'eta': 50.5}, mean: 0.95556, std: 0.03995, params: {'eta': 75.25}, mean: 0.95556, std: 0.03995, params: {'eta': 100.0}]\n",
      "Training error: 0.0\n",
      "Testing error: 0.0666666666667\n",
      "##############################################################\n",
      "\n",
      "\n",
      "##############################################################\n",
      "Dataset: 11\n",
      "Best parameter: {'eta': 1.0}\n",
      "CV scores:\n",
      "[mean: 0.91111, std: 0.05814, params: {'eta': 1.0}, mean: 0.91111, std: 0.05814, params: {'eta': 25.75}, mean: 0.91111, std: 0.05814, params: {'eta': 50.5}, mean: 0.91111, std: 0.05814, params: {'eta': 75.25}, mean: 0.91111, std: 0.05814, params: {'eta': 100.0}]\n",
      "Training error: 0.0\n",
      "Testing error: 0.166666666667\n",
      "##############################################################\n",
      "\n",
      "\n",
      "##############################################################\n",
      "Dataset: 12\n",
      "Best parameter: {'eta': 1.0}\n",
      "CV scores:\n",
      "[mean: 0.94444, std: 0.05120, params: {'eta': 1.0}, mean: 0.94444, std: 0.05120, params: {'eta': 25.75}, mean: 0.94444, std: 0.05120, params: {'eta': 50.5}, mean: 0.94444, std: 0.05120, params: {'eta': 75.25}, mean: 0.94444, std: 0.05120, params: {'eta': 100.0}]\n",
      "Training error: 0.0333333333333\n",
      "Testing error: 0.2\n",
      "##############################################################\n",
      "\n",
      "\n",
      "##############################################################\n",
      "Dataset: 13\n",
      "Best parameter: {'eta': 1.0}\n",
      "CV scores:\n",
      "[mean: 0.92222, std: 0.07317, params: {'eta': 1.0}, mean: 0.92222, std: 0.07317, params: {'eta': 25.75}, mean: 0.92222, std: 0.07317, params: {'eta': 50.5}, mean: 0.92222, std: 0.07317, params: {'eta': 75.25}, mean: 0.92222, std: 0.07317, params: {'eta': 100.0}]\n",
      "Training error: 0.0\n",
      "Testing error: 0.166666666667\n",
      "##############################################################\n",
      "\n",
      "\n",
      "##############################################################\n",
      "Dataset: 14\n",
      "Best parameter: {'eta': 1.0}\n",
      "CV scores:\n",
      "[mean: 0.91111, std: 0.06697, params: {'eta': 1.0}, mean: 0.91111, std: 0.06697, params: {'eta': 25.75}, mean: 0.91111, std: 0.06697, params: {'eta': 50.5}, mean: 0.91111, std: 0.06697, params: {'eta': 75.25}, mean: 0.91111, std: 0.06697, params: {'eta': 100.0}]\n",
      "Training error: 0.0\n",
      "Testing error: 0.0333333333333\n",
      "##############################################################\n",
      "\n",
      "\n",
      "##############################################################\n",
      "Dataset: 15\n",
      "Best parameter: {'eta': 1.0}\n",
      "CV scores:\n",
      "[mean: 0.93333, std: 0.08166, params: {'eta': 1.0}, mean: 0.93333, std: 0.08166, params: {'eta': 25.75}, mean: 0.93333, std: 0.08166, params: {'eta': 50.5}, mean: 0.93333, std: 0.08166, params: {'eta': 75.25}, mean: 0.93333, std: 0.08166, params: {'eta': 100.0}]\n",
      "Training error: 0.0444444444444\n",
      "Testing error: 0.133333333333\n",
      "##############################################################\n",
      "\n",
      "\n",
      "##############################################################\n",
      "Dataset: 16\n",
      "Best parameter: {'eta': 1.0}\n",
      "CV scores:\n",
      "[mean: 0.92222, std: 0.02722, params: {'eta': 1.0}, mean: 0.92222, std: 0.02722, params: {'eta': 25.75}, mean: 0.92222, std: 0.02722, params: {'eta': 50.5}, mean: 0.92222, std: 0.02722, params: {'eta': 75.25}, mean: 0.92222, std: 0.02722, params: {'eta': 100.0}]\n",
      "Training error: 0.0\n",
      "Testing error: 0.166666666667\n",
      "##############################################################\n",
      "\n",
      "\n",
      "##############################################################\n",
      "Dataset: 17\n",
      "Best parameter: {'eta': 1.0}\n",
      "CV scores:\n",
      "[mean: 0.94444, std: 0.04969, params: {'eta': 1.0}, mean: 0.94444, std: 0.04969, params: {'eta': 25.75}, mean: 0.94444, std: 0.04969, params: {'eta': 50.5}, mean: 0.94444, std: 0.04969, params: {'eta': 75.25}, mean: 0.94444, std: 0.04969, params: {'eta': 100.0}]\n",
      "Training error: 0.0\n",
      "Testing error: 0.166666666667\n",
      "##############################################################\n",
      "\n",
      "\n",
      "##############################################################\n",
      "Dataset: 18\n",
      "Best parameter: {'eta': 1.0}\n",
      "CV scores:\n",
      "[mean: 0.92222, std: 0.05773, params: {'eta': 1.0}, mean: 0.92222, std: 0.05773, params: {'eta': 25.75}, mean: 0.92222, std: 0.05773, params: {'eta': 50.5}, mean: 0.92222, std: 0.05773, params: {'eta': 75.25}, mean: 0.92222, std: 0.05773, params: {'eta': 100.0}]\n",
      "Training error: 0.0333333333333\n",
      "Testing error: 0.0333333333333\n",
      "##############################################################\n",
      "\n",
      "\n",
      "##############################################################\n",
      "Dataset: 19\n",
      "Best parameter: {'eta': 1.0}\n",
      "CV scores:\n",
      "[mean: 0.92222, std: 0.05748, params: {'eta': 1.0}, mean: 0.92222, std: 0.05748, params: {'eta': 25.75}, mean: 0.92222, std: 0.05748, params: {'eta': 50.5}, mean: 0.92222, std: 0.05748, params: {'eta': 75.25}, mean: 0.92222, std: 0.05748, params: {'eta': 100.0}]\n",
      "Training error: 0.0333333333333\n",
      "Testing error: 0.1\n",
      "##############################################################\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "global name 'winners' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-67ba3d560fd7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msolve_clf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'PP'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'credit'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_func\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-12-e155ae36a360>\u001b[0m in \u001b[0;36msolve_clf\u001b[1;34m(algorithm, dataset, params, grid, data_func, show)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mparams\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m         \u001b[0mmake_hist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwinners\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: global name 'winners' is not defined"
     ]
    }
   ],
   "source": [
    "solve_clf('PP', 'credit', params=params, data_func=normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine with Linear Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 2 - Pima Indians Diabetes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anexo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuración del notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#notebook settings\n",
    "%matplotlib inline\n",
    "\n",
    "#import some useful libraries and utilities\n",
    "import numba\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "#setting some paths\n",
    "path1 = './credit/'\n",
    "#data directory\n",
    "path2 = './diabetes/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funciones para el manejo de la data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Rescale features of M to [a,b] range\n",
    "def rescale(M, a=0., b=1.):\n",
    "    #max and min vectors\n",
    "    maxv = np.max(M, axis=0)\n",
    "    minv = np.min(M, axis=0)\n",
    "    return (b-a)*M/(maxv-minv) + (a*maxv-b*minv)/(maxv-minv)\n",
    "\n",
    "#Normalize features of M\n",
    "def normalize(M):\n",
    "    #mean and standard deviation vectors\n",
    "    meanv = np.mean(M, axis=0)\n",
    "    stdv = np.std(M, axis=0)\n",
    "    return (M-meanv)/stdv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Métricas de error para clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#precision\n",
    "def precision(yreal, ypred):\n",
    "    yr = yreal.astype(int)\n",
    "    yp = ypred.astype(int)\n",
    "    m, = yr.shape\n",
    "    #number of good predictions/number of predictions\n",
    "    return np.sum(yp==yr)/np.float(m)\n",
    "\n",
    "#error rate \n",
    "def error_rate(yreal, ypred):\n",
    "    return 1.-precision(yreal, ypred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementación de LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LDA(BaseEstimator,ClassifierMixin):\n",
    "    def __init_(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        #boolean masks\n",
    "        mask1 = y.astype(bool)\n",
    "        mask0 = np.logical_not(mask1)\n",
    "        #number of samples in each class\n",
    "        M,_ = X.shape\n",
    "        M0 = np.sum(mask0)\n",
    "        M1 = M-M0\n",
    "        #estimates of probabilities\n",
    "        p0 = M0/np.float(M)\n",
    "        p1 = 1-p0\n",
    "        #estimates of means\n",
    "        u0 = X[mask0].sum(axis=0)/M0\n",
    "        u1 = X[mask1].sum(axis=0)/M1\n",
    "        #estimation of shared covariance matrix\n",
    "        Y = X.copy()\n",
    "        Y[mask0] -= u0\n",
    "        Y[mask1] -= u1\n",
    "        Cov = np.dot(Y.T,Y)\n",
    "        Cov /= (M-2.)\n",
    "        #computing parameters of linear discriminants\n",
    "        self.w0 = np.linalg.solve(Cov,u0)\n",
    "        self.b0 = np.log(p0) - 0.5*np.dot(u0, self.w0)\n",
    "        self.w1 = np.linalg.solve(Cov,u1)\n",
    "        self.b1 = np.log(p1) - 0.5*np.dot(u1, self.w1)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        #evaluation of discriminant hyperplane\n",
    "        return (np.dot(X,self.w1-self.w0) > self.b0-self.b1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementación de Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-5-7c8350f94e99>, line 68)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-5-7c8350f94e99>\"\u001b[1;36m, line \u001b[1;32m68\u001b[0m\n\u001b[1;33m    else:\u001b[0m\n\u001b[1;37m       ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "class NB(BaseEstimator,ClassifierMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y, k, features):\n",
    "        #boolean masks\n",
    "        mask1 = y.astype(bool)\n",
    "        mask0 = np.logical_not(mask1)\n",
    "        #number of samples in each class\n",
    "        M,N = X.shape\n",
    "        M0 = np.sum(mask0)\n",
    "        M1 = M-M0\n",
    "        #estimates of probabilities\n",
    "        p0 = M0/np.float(M)\n",
    "        p1 = 1-p0\n",
    "        #separating data by classes\n",
    "        X0 = X[mask0]\n",
    "        X1 = X[mask1]\n",
    "        #conditional probabilities of features in each class\n",
    "        CP0 = list()\n",
    "        CP1 = list()\n",
    "        #matrix of bins for each feature\n",
    "        B0 = np.emtpy((len(features),k+1))\n",
    "        B1 = np.empty((len(features),k+1))\n",
    "        ind = 0\n",
    "        \n",
    "        #tolerance on interval extremes of discretization\n",
    "        eps = 1e-10\n",
    "        \n",
    "        #computing conditional probabilities of features\n",
    "        for f in xrange(N):\n",
    "            #discretization needed\n",
    "            if f in features:\n",
    "                tmp0 = X0[:,f]\n",
    "                tmp1 = X1[:,f]\n",
    "                bins0 = np.linspace(tmp0.min()-eps, tmp0.max()+eps, k+1, endpoint=True)\n",
    "                bins1 = np.linspace(tmp1.min()-eps, tmp1.max()+eps, k+1, endpoint=True)\n",
    "                CP0.append(np.bincount(np.digitize(tmp0, bins0), minlength=k+2)/M0)\n",
    "                CP1.append(np.bincount(np.digitize(tmp1, bins1), minlength=k+2)/M1)\n",
    "                B0[ind] = bins0\n",
    "                B1[ind] = bins1\n",
    "                ind += 1\n",
    "            #no discretization needed\n",
    "            else:\n",
    "                tmp0 = X0[:,f].astype(int)\n",
    "                tmp1 = X1[:,f].astype(int)\n",
    "                CP0.append(np.bincount(tmp0, minlength=np.max(tmp0)+2)/M0)\n",
    "                CP1.append(np.bincount(tmp1, minlength=np.max(tmp1)+2)/M1)\n",
    "                \n",
    "        #storing needed data\n",
    "        self.features = features\n",
    "        self.p0 = p0\n",
    "        self.p1 = p1 \n",
    "        self.CP0 = CP0\n",
    "        self.CP1 = CP1\n",
    "        self.B0 = B0\n",
    "        self.B1 = B1\n",
    "        \n",
    "    def predict(self, X):\n",
    "        M,N = X.shape\n",
    "        scores0 = np.ones(M)\n",
    "        scores1 = np.ones(M)\n",
    "        for f in xrange(N):\n",
    "            ##discretization needed\n",
    "            if f in self.features:\n",
    "                \n",
    "            #no discretization needed\n",
    "            else:\n",
    "        \n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementación de Primal Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class PP(BaseEstimator,ClassifierMixin):\n",
    "    def __init__(self, eta=0.01, n_iter=100000, tol=25):\n",
    "        self.eta = eta\n",
    "        self.n_iter = n_iter\n",
    "        self.tol = tol\n",
    "    \n",
    "    def f(self, X):\n",
    "        return np.dot(X,self.w)+self.b\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        M,N = X.shape\n",
    "        #initial guesses\n",
    "        self.w = np.zeros(N)\n",
    "        self.b = 0.\n",
    "        #R parameter\n",
    "        R = np.max(np.linalg.norm(X, axis=1))\n",
    "        #error rate and counter\n",
    "        err1 = error_rate(y, self.predict(X))\n",
    "        c = 0\n",
    "        \n",
    "        for i in xrange(self.n_iter):\n",
    "            #error rate at previous iteration\n",
    "            err0 = err1\n",
    "            for m in xrange(M):\n",
    "                #if misclassified\n",
    "                if y[m]*self.f(X[m])<=0:\n",
    "                    self.w += self.eta*y[m]*X[m]\n",
    "                    self.b += self.eta*y[m]*R**2\n",
    "            err1 = error_rate(y, self.predict(X))\n",
    "            #stopping criterions\n",
    "            if err1==0.: break\n",
    "            elif err0==err1:\n",
    "                c += 1\n",
    "                if c > self.tol: break\n",
    "            else: c = 0\n",
    "        #store the number of iterations needed\n",
    "        self.it = i+1\n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return np.sign(self.f(X))\n",
    "    \n",
    "    #to make it compatible with scikit\n",
    "    def score(self, X, y):\n",
    "        return precision(y, self.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ La primera vez que se implemento, en la regla de actualización para $b$ se puso equívocamente: $b \\leftarrow b + \\nu y_m R \\ $    en vez de $R^2$, y los resultados obtenidos fueron mucho mejores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wa = np.load(path1+'credit-tr-1.npy')\n",
    "X = normalize(wa[:,:-1])\n",
    "y = wa[:,-1]\n",
    "y[y==0.]=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=PP(eta=0.01, n_iter=100000, tol=25), fit_params={},\n",
       "       iid=True, n_jobs=2,\n",
       "       param_grid={'eta': array([  0.01  ,   2.5075,   5.005 ,   7.5025,  10.    ])},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp = PP()\n",
    "params = {'eta':np.linspace(0.01, 10., 5, endpoint=True)}\n",
    "gs = GridSearchCV(pp, params, cv=5, n_jobs=2)\n",
    "gs.fit(X, y)\n",
    "#clf.fit(X, y)\n",
    "#print clf.score(X,y)\n",
    "#print clf.b\n",
    "#print clf.w\n",
    "#print clf.it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[mean: 0.92222, std: 0.07565, params: {'eta': 0.01},\n",
       " mean: 0.92222, std: 0.07565, params: {'eta': 2.5074999999999998},\n",
       " mean: 0.92222, std: 0.07565, params: {'eta': 5.0049999999999999},\n",
       " mean: 0.92222, std: 0.07565, params: {'eta': 7.5024999999999995},\n",
       " mean: 0.92222, std: 0.07565, params: {'eta': 10.0}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eta': 0.01}\n"
     ]
    }
   ],
   "source": [
    "print gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementación de Dual Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DP(BaseEstimator,ClassifierMixin):\n",
    "    def __init__(self, eta=0.01, gamma=0.5, n_iter=100000, tol=20):\n",
    "        self.eta = eta\n",
    "        self.gamma = gamma\n",
    "        self.n_iter = n_iter\n",
    "        self.tol = tol\n",
    "        pass\n",
    "\n",
    "    numba.jit()\n",
    "    def fit(self, X, y):\n",
    "        M,N = X.shape\n",
    "        #store data\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        #initial guesses\n",
    "        self.alp = np.zeros(M)\n",
    "        self.b = 0.\n",
    "        #R parameter\n",
    "        R = np.max(np.linalg.norm(X, axis=1))\n",
    "        #precomputed kernel matrix\n",
    "        self.K = rbf_kernel(X, gamma=self.gamma)\n",
    "        #error rate and counter\n",
    "        err1 = error_rate(y, self.predict(X))\n",
    "        c = 0\n",
    "        \n",
    "        for i in xrange(self.n_iter):\n",
    "            #error rate at previous iteration\n",
    "            err0 = err1\n",
    "            for m in xrange(M):\n",
    "                #if misclassified\n",
    "                if y[m]*(np.sum(self.alp*y*self.K[m])+self.b)<=0:\n",
    "                    self.alp[m] += 1\n",
    "                    self.b += self.eta*y[m]*R**2\n",
    "            err1 = error_rate(y, self.predict(X))\n",
    "            #stopping criterions\n",
    "            if err1==0.: break\n",
    "            elif err0==err1:\n",
    "                c += 1\n",
    "                if c > self.tol: break\n",
    "            else: c = 0\n",
    "        #store the number of iterations needed\n",
    "        self.it = i+1\n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        if np.array_equal(X, self.X):\n",
    "            K = self.K\n",
    "        else:\n",
    "            #compute the kernel matrix\n",
    "            K = rbf_kernel(X, self.X, self.gamma)\n",
    "        f = np.sum(self.alp*self.y*K, axis=1)+self.b\n",
    "        return np.sign(f)\n",
    "    \n",
    "    #to make it compatible with GridSearch\n",
    "    def score(self, X, y):\n",
    "        return precision(y, self.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wa = np.load(path2+'diabetes-tr-0.npy')\n",
    "X = normalize(wa[:,:-1])\n",
    "y = wa[:,-1]\n",
    "y[y==0.]=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DP(eta=1.0, gamma=0.5, n_iter=100000, tol=10)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp = DP(gamma=0.5, eta=1.)\n",
    "dp.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=DP(eta=0.01, gamma=0.5, n_iter=100000, tol=20),\n",
       "       fit_params={}, iid=True, n_jobs=4,\n",
       "       param_grid={'eta': array([  1.  ,   3.25,   5.5 ,   7.75,  10.  ]), 'gamma': array([ 0.5  ,  0.875,  1.25 ,  1.625,  2.   ])},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp = DP()\n",
    "params = {'gamma':np.linspace(0.5, 2., 5, endpoint=True), 'eta':np.linspace(1., 10., 5, endpoint=True)}\n",
    "gs = GridSearchCV(dp, params, cv=5, n_jobs=4)\n",
    "gs.fit(X, y)\n",
    "#dp = DP()\n",
    "#dp.fit(X, y)\n",
    "#print dp.b\n",
    "#print dp.it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[mean: 0.69271, std: 0.03488, params: {'eta': 1.0, 'gamma': 0.5},\n",
       " mean: 0.69618, std: 0.01302, params: {'eta': 1.0, 'gamma': 0.875},\n",
       " mean: 0.68229, std: 0.02227, params: {'eta': 1.0, 'gamma': 1.25},\n",
       " mean: 0.69271, std: 0.01106, params: {'eta': 1.0, 'gamma': 1.625},\n",
       " mean: 0.68056, std: 0.02293, params: {'eta': 1.0, 'gamma': 2.0},\n",
       " mean: 0.69965, std: 0.03112, params: {'eta': 3.25, 'gamma': 0.5},\n",
       " mean: 0.69618, std: 0.01302, params: {'eta': 3.25, 'gamma': 0.875},\n",
       " mean: 0.68229, std: 0.02227, params: {'eta': 3.25, 'gamma': 1.25},\n",
       " mean: 0.69271, std: 0.01106, params: {'eta': 3.25, 'gamma': 1.625},\n",
       " mean: 0.68056, std: 0.02293, params: {'eta': 3.25, 'gamma': 2.0},\n",
       " mean: 0.69965, std: 0.03112, params: {'eta': 5.5, 'gamma': 0.5},\n",
       " mean: 0.69618, std: 0.01302, params: {'eta': 5.5, 'gamma': 0.875},\n",
       " mean: 0.68229, std: 0.02227, params: {'eta': 5.5, 'gamma': 1.25},\n",
       " mean: 0.69271, std: 0.01106, params: {'eta': 5.5, 'gamma': 1.625},\n",
       " mean: 0.68056, std: 0.02293, params: {'eta': 5.5, 'gamma': 2.0},\n",
       " mean: 0.69965, std: 0.03112, params: {'eta': 7.75, 'gamma': 0.5},\n",
       " mean: 0.69618, std: 0.01302, params: {'eta': 7.75, 'gamma': 0.875},\n",
       " mean: 0.68229, std: 0.02227, params: {'eta': 7.75, 'gamma': 1.25},\n",
       " mean: 0.69271, std: 0.01106, params: {'eta': 7.75, 'gamma': 1.625},\n",
       " mean: 0.68056, std: 0.02293, params: {'eta': 7.75, 'gamma': 2.0},\n",
       " mean: 0.69965, std: 0.03112, params: {'eta': 10.0, 'gamma': 0.5},\n",
       " mean: 0.69618, std: 0.01302, params: {'eta': 10.0, 'gamma': 0.875},\n",
       " mean: 0.68229, std: 0.02227, params: {'eta': 10.0, 'gamma': 1.25},\n",
       " mean: 0.69271, std: 0.01106, params: {'eta': 10.0, 'gamma': 1.625},\n",
       " mean: 0.68056, std: 0.02293, params: {'eta': 10.0, 'gamma': 2.0}]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eta': 3.25, 'gamma': 0.5}\n"
     ]
    }
   ],
   "source": [
    "print gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wa = np.load(path2+'diabetes-tr-10.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 3 3 4 3 3 5 3 3 3 4 3 4 4 3 4 3 4 3 2 3 3 3 3 4 3 3 3 3 4 3 4 3 3 3 3 3\n",
      " 4 4 3 3 3 3 3 3 3 3 4 3 3 3 3 3 4 3 5 3 3 3 3 3 4 4 4 5 4 3 4 5 3 3 3 4 3\n",
      " 3 2 3 4 5 3 3 3 5 3 3 2 4 3 3 2 3 3 2 4 4 4 5 3 4 4 3 4 4 4 3 4 4 3 5 3 3\n",
      " 3 4 3 3 3 4 3 3 4 4 4 3 4 2 3 5 2 2 4 3 3 3 3 3 3 3 3 3 3 4 2 2 4 4 4 3 4\n",
      " 4 2 3 3 3 5 3 3 4 4 3 4 3 4 4 3 4 3 3 3 3 3 3 2 3 4 3 3 4 4 3 3 4 2 3 4 3\n",
      " 4 2 4 3 3 3 3 3 3 4 3 3 3 3 2 5 3 4 3 5 3 3 3 3 4 3 4 4 2 4 4 3 5 3 4 3 4\n",
      " 3 4 4 3 3 4 4 4 3 3 3 3 3 3 2 4 4 4 3 3 3 4 3 3 3 3 3 4 3 4 2 3 4 3 4 4 4\n",
      " 3 2 4 3 3 4 4 3 4 3 4 3 4 3 3 3 4 1 2 3 3 4 3 3 4 3 3 3 5 5 2 3 4 2 3 3 4\n",
      " 4 4 3 3 4 3 3 4 3 3 2 3 3 4 1 3 3 4 3 3 3 3 3 3 3 4 3 3 3 4 2 3 3 3 4 3 3\n",
      " 2 3 3 3 3 5 2 5 2 3 4 3 3 4 3 4 3 4 4 4 3 3 3 3 3 4 3 3 3 3 3 4 4 4 3 3 4\n",
      " 4 3 3 3 3 3 2 3 3 4 4 3 3 5]\n",
      "[  0   2  28 221 115  18   0]\n"
     ]
    }
   ],
   "source": [
    "wa = np.load(path2+'diabetes-tr-10.npy')\n",
    "X = wa[:,:-1]\n",
    "y = wa[:,-1]\n",
    "#boolean masks\n",
    "mask1 = y.astype(bool)\n",
    "mask0 = np.logical_not(mask1)\n",
    "#number of samples in each class\n",
    "M,_ = X.shape\n",
    "M0 = np.sum(mask0)\n",
    "M1 = M-M0\n",
    "#estimates of probabilities\n",
    "p0 = M0/np.float(M)\n",
    "p1 = 1-p0\n",
    "#separating data by class\n",
    "X0 = X[mask0]\n",
    "X1 = X[mask1]\n",
    "\n",
    "f = X0[:,1]\n",
    "eps = 1e-10\n",
    "bins = np.linspace(t.min()-eps,t.max()+eps, 6, endpoint=True)\n",
    "print np.digitize(f, bins)\n",
    "print np.bincount(np.digitize(f, bins), minlength=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "> find the best parameter for classification algorithms, between \n",
    "  parameters in params using 5-fold cross validation.\n",
    "> for algorithm with 1 parameter\n",
    "\"\"\"\n",
    "def cross(clf, X, y, params):\n",
    "    #creating kfold\n",
    "    m,n = X.shape\n",
    "    kf = KFold(m, n_folds=5)\n",
    "    cv_err = np.empty((5,5))\n",
    "    i = 0 #index of fold\n",
    "    \n",
    "    #iterate through folds\n",
    "    for tr_index,ts_index in kf:\n",
    "        j = 0 #index of parameter\n",
    "        X_tr, X_ts = X[tr_index], X[ts_index]\n",
    "        y_tr, y_ts = y[tr_index], y[ts_index]\n",
    "        #iterate through parameters to try\n",
    "        for param in params:\n",
    "            clf.fit(X_tr, y_tr, param)\n",
    "            cv_err[i,j] = error_rate(y_ts, clf.predict(X_ts))\n",
    "            j += 1\n",
    "        i += 1\n",
    "        \n",
    "    #arrays with mean cv-error for each alpha\n",
    "    cv_mean = np.mean(cv_err, axis=0)\n",
    "    return params[np.argmin(cv_mean)], cv_err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones complementarias (helpers) para obtener resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function to generate histogram of winners\n",
    "\"\"\"\n",
    "def make_hist(winners,params):\n",
    "    winners = np.array(winners)\n",
    "    freqs = np.zeros(5)\n",
    "    for i in xrange(5):\n",
    "        freqs[i] = np.sum(params[i]==winners)\n",
    "\n",
    "    labels = map(str,params)\n",
    "    pos = np.arange(len(labels))\n",
    "    width = 1.0\n",
    "    fig = plt.figure()\n",
    "    fig.set_figheight(5)\n",
    "    fig.set_figwidth(5)\n",
    "    ax = plt.axes()\n",
    "    ax.set_xticks(pos + (width / 2))\n",
    "    ax.set_xticklabels(labels)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Histogram of winner parameters')\n",
    "    plt.bar(pos, freqs, width, color='0.5')\n",
    "    plt.show()\n",
    "\n",
    "\"\"\"\n",
    "Generate solutions for classification problems\n",
    "\"\"\"\n",
    "def solve_clf(algorithm, dataset, params=None, grid=None, data_func=None, show=None):\n",
    "    #if params is not None:\n",
    "    #    winners = list()\n",
    "        \n",
    "    #set dataset name\n",
    "    if dataset=='credit':\n",
    "        path = './credit/credit'\n",
    "    elif dataset=='diabetes':\n",
    "        path = './diabetes/diabetes'\n",
    "    else: return -1\n",
    "    \n",
    "    #set classifier\n",
    "    if algorithm=='LDA':   clf = LDA()\n",
    "    elif algorithm=='NB':  clf = NB()\n",
    "    elif algorithm=='PP':  clf = PP()\n",
    "    elif algorithm=='DP':  clf = DP()\n",
    "    elif algorithm=='SVM': clf = svm.SVC()\n",
    "    else: return -1\n",
    "    \n",
    "    #iterate through data\n",
    "    for i in xrange(20):\n",
    "        #loading dataset\n",
    "        tr_file = path+'-tr-{0}.npy'.format(i)\n",
    "        ts_file = path+'-ts-{0}.npy'.format(i)\n",
    "        tr_data = np.load(tr_file)\n",
    "        ts_data = np.load(ts_file)\n",
    "        \n",
    "        if data_func is not None:\n",
    "            X_tr = data_func(tr_data[:,:-1])\n",
    "            X_ts = data_func(ts_data[:,:-1])\n",
    "        else: \n",
    "            X_tr = tr_data[:,:-1]\n",
    "            X_ts = ts_data[:,:-1]\n",
    "        y_tr = np.ascontiguousarray(tr_data[:,-1])\n",
    "        y_ts = np.ascontiguousarray(ts_data[:,-1])\n",
    "        if algorithm in ['PP', 'DP', 'SVM']:\n",
    "            #change label for separating hyperplanes approaches\n",
    "            y_tr[y_tr==0.] = -1\n",
    "            y_ts[y_ts==0. ]= -1\n",
    "\n",
    "        if params is not None:\n",
    "            #tunning parameters\n",
    "            gs = GridSearchCV(clf, params, cv=5, n_jobs=4)\n",
    "            gs.fit(X_tr, y_tr)\n",
    "            grid_scores = gs.grid_scores_\n",
    "            best_params = gs.best_params_\n",
    "            #train the data \n",
    "            clf.set_params(**best_params)\n",
    "            clf.fit(X_tr, y_tr)\n",
    "        else:\n",
    "            clf.fit(X_tr, y_tr)\n",
    "        \n",
    "        if (show is not None) and (i not in show): continue\n",
    "        print \"##############################################################\"\n",
    "        print \"Dataset: {0}\".format(i)\n",
    "        if params is not None:\n",
    "            print 'Best parameter: {0}'.format(best_params)\n",
    "            print 'CV scores:'\n",
    "            print grid_scores\n",
    "        print 'Training error: {0}'.format(error_rate(y_tr, clf.predict(X_tr)))\n",
    "        print 'Testing error: {0}'.format(error_rate(y_ts, clf.predict(X_ts)))\n",
    "        print \"##############################################################\"\n",
    "        print '\\n'\n",
    "    \n",
    "    #if params is not None:\n",
    "        #make_hist(winners,params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
