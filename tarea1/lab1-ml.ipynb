{
 "metadata": {
  "name": "",
  "signature": "sha256:ba190595e5e72ed4d7af63dc0df52192aa92cee13ad78c0e8da45d9f86c2de0d"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Parte 1"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#notebook settings\n",
      "%matplotlib inline\n",
      "\n",
      "#import some useful libraries and utilities\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.cross_validation import KFold\n",
      "\n",
      "#setting some paths\n",
      "path1='./cereales/'\n",
      "path2='./credit/'\n",
      "\n",
      "#parameters to try\n",
      "params = np.linspace(0.002,0.01,5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Overall cost function for linear regresion\n",
      "def J(X, y, beta):\n",
      "    h = np.dot(X,beta)\n",
      "    loss = h-y\n",
      "    return 0.5*np.dot((loss,loss)\n",
      "\n",
      "#Implementation of batch gradient descent for linear regression\n",
      "def gd_batch(X, y, alpha=0.5, eps=1e-5, max_iter=1000000):\n",
      "    m,n = X.shape\n",
      "    beta = np.zeros(n)\n",
      "    J1 = J(X,y,beta) #loss at previous iteration\n",
      "    for i in range(max_iter):\n",
      "        J0 = J1\n",
      "        h = np.dot(X,beta)\n",
      "        dJ = np.dot(X.T,h-y)\n",
      "        beta -= alpha*dJ\n",
      "        J1 = J(X,y,beta)\n",
      "        if np.abs(J1-J0)/J0 < eps: break\n",
      "    return beta\n",
      "\n",
      "#Implementation of online gradient descent for linear regression\n",
      "def gd_online(X, y, alpha=0.5, eps=1e-5, max_iter=1000000):\n",
      "    m,n = X.shape\n",
      "    beta = np.zeros(n)\n",
      "    J1 = J(X,y,beta) #loss at previous iteration\n",
      "    for i in range(max_iter):\n",
      "        J0 = J1\n",
      "        for i in range(m):\n",
      "            beta -= alpha*(np.dot(X[i],beta)-y[i])*X[i]\n",
      "        J1 = J(X,y,beta)\n",
      "        if np.abs(J1-J0)/J0 < eps: break\n",
      "    return beta\n",
      "\n",
      "#Implementation of Newton-Raphson method for linear regression\n",
      "def nr_linear(X, y, eps=1e-5, max_iter=1000000):\n",
      "    m,n = X.shape\n",
      "    beta = np.zeros(n)\n",
      "    J1 = J(X,y,beta) #loss at previous iteration \n",
      "    Hess = np.dot(X.T,X) #Hessian matrix\n",
      "    for i in range(max_iter):\n",
      "        J0 = J1\n",
      "        h = np.dot(X,beta)\n",
      "        dJ = np.dot(X.T,h-y)\n",
      "        beta -= np.linalg.solve(Hess, dJ)\n",
      "        J1 = J(X,y,beta)\n",
      "        if np.abs(J1-J0)/J0 < eps: break\n",
      "    return beta\n",
      "\n",
      "#Implementation of weighted gradient descent\n",
      "def gd_weighted(X, y):\n",
      "    return\n",
      "\n",
      "def rescale(M,a,b):\n",
      "    \"\"\" Rescale features of M to [a,b] range \"\"\"\n",
      "    #max and min vectors\n",
      "    maxv = np.max(M, axis=0)\n",
      "    minv = np.min(M, axis=0)\n",
      "    return (b-a)*M/(maxv-minv) + (a*maxv-b*minv)/(maxv-minv)\n",
      "\n",
      "def normalize(M):\n",
      "    #mean and standard deviation vectors\n",
      "    meanv = np.mean(M, axis=0)\n",
      "    stdv = np.std(M, axis=0)\n",
      "    return (M-meanv)/stdv"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 52
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#working array\n",
      "wa = np.load(path1+'cereales-tr-0.npy')\n",
      "rescaled_wa = rescale(wa,0.,1.)\n",
      "normalized_wa = normalize(wa)\n",
      "X = rescaled_wa[:,:-1]\n",
      "y = rescaled_wa[:,-1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 53
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Xa = wa[:,:-1]\n",
      "ya = wa[:,-1]\n",
      "betaa = gd_batch(Xa,ya,alpha=1e-7)\n",
      "print betaa\n",
      "print J(Xa,ya,betaa)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 0.02599391  0.97004858 -1.13311795 -0.06157265  1.3013288   2.66707282\n",
        " -0.56670881  0.08493078 -0.0879737   0.69604403  0.20786619  0.38640436]\n",
        "3722.58867348\n"
       ]
      }
     ],
     "prompt_number": 54
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "beta1 = gd_batch(X, y, alpha=0.005)\n",
      "print 'beta:',beta1\n",
      "print 'error:',J(X,y,beta1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "beta: [-0.29058364  0.23575754 -0.1717127  -0.19528326  0.73328694  0.28899906\n",
        " -0.17079539 -0.18881757 -0.19586133  0.11726998  0.35186727  0.5275573 ]\n",
        "error: 0.172991850589\n"
       ]
      }
     ],
     "prompt_number": 56
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "beta2 = gd_online(X, y, alpha=0.005)\n",
      "print 'beta:',beta2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "beta: [-0.284187    0.24098783 -0.18022872 -0.19401827  0.66518596  0.29444591\n",
        " -0.16144881 -0.12269884 -0.18915525  0.11251132  0.32425563  0.52679119]\n"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "beta3 = nr_linear(X,y)\n",
      "print 'beta:',beta3"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "beta: [-0.28477401  0.23382422 -0.1711402  -0.19611558  0.766077    0.28185174\n",
        " -0.17850135 -0.22100888 -0.19930546  0.11996579  0.36355978  0.52893807]\n"
       ]
      }
     ],
     "prompt_number": 36
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\" Find the best learning parameter for algorithm, between \n",
      "parameters in params using k-fold cross validation \"\"\"\n",
      "def find_best(X, y, algorithm, params, mode=None):\n",
      "    #creating kfold\n",
      "    m,n = X.shape\n",
      "    kf = KFold(m, n_folds=5)\n",
      "    tr_cost = list()\n",
      "    ts_cost = list()\n",
      "    \n",
      "    for param in params:\n",
      "        mean_tr_cost = 0\n",
      "        mean_ts_cost = 0\n",
      "        for tr_index,ts_index in kf:\n",
      "            X_train, X_test = X[tr_index], X[ts_index]\n",
      "            y_train, y_test = y[tr_index], y[ts_index]\n",
      "            beta = algorithm(X_train, y_train, alpha=param)\n",
      "            mean_tr_cost += J(X_train, y_train, beta)\n",
      "            mean_ts_cost += J(X_test, y_test, beta)\n",
      "        tr_cost.append(mean_tr_cost/5)\n",
      "        ts_cost.append(mean_ts_cost/5)\n",
      "    if mode=='verbose':\n",
      "        #print some info\n",
      "        print 'Mean training errors for each alpha:'\n",
      "        print tr_cost\n",
      "        print 'Mean testing errors for each alpha:'\n",
      "        print ts_cost\n",
      "    return params[np.argmin(np.array(ts_cost))]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "find_best(X, y, gd_batch, params, mode='verbose')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Mean training errors for each alpha:\n",
        "[0.14346357539067406, 0.13749300306475862, 0.13554424716673849, 0.13457225314625129, 0.13399356971691684]\n",
        "Mean testing errors for each alpha:\n",
        "[0.054066821646794251, 0.052022098345865055, 0.051443667229546394, 0.051186865059925687, 0.051054242620090715]\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 41,
       "text": [
        "0.01"
       ]
      }
     ],
     "prompt_number": 41
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for i in range(20):\n",
      "    tr_file = path1+'cereales-tr-{0}.npy'.format(i)\n",
      "    ts_file = path1+'cereales-ts-{0}.npy'.format(i)\n",
      "    tr_data = np.load(tr_file)\n",
      "    ts_data = np.load(ts_file)\n",
      "    X_tr = rescale(tr_data[:,:-1], 0., 1.)\n",
      "    y_tr = tr_data[:,-1]\n",
      "    X_ts = rescale(ts_data[:,:-1], 0., 1.)\n",
      "    y_ts = ts_data[:,-1]\n",
      "    alpha = find_best(X_tr, y_tr, gd_batch, params)\n",
      "    beta = gd_batch(X_tr, y_tr, alpha)\n",
      "    print 'Best alpha:',alpha,'Training error:',J(X_tr,y_tr,beta),'Testing error:',J(X_ts,y_ts,beta)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Best alpha: 0.01 Training error: 2071.04108657 Testing error: 2484.76100314\n",
        "Best alpha:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.01 Training error: 2000.6253735 Testing error: 2295.26584449\n",
        "Best alpha:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.01 Training error: 2781.38270249 Testing error: 3900.70227886\n",
        "Best alpha:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.01 Training error: 2675.56924202 Testing error: 3905.75511688\n",
        "Best alpha:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.01 Training error: 1834.00171907 Testing error: 1804.39319415\n",
        "Best alpha:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.004 Training error: 1616.7222372 Testing error: 2386.38191037\n",
        "Best alpha:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.01 Training error: 1974.21009071 Testing error: 5041.79565741\n",
        "Best alpha:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.008 Training error: 1898.07237923 Testing error: 1869.98188998\n",
        "Best alpha:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.01 Training error: 2154.11535571 Testing error: 2100.74411243\n",
        "Best alpha:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.01 Training error: 1992.80711704 Testing error: 8974.27027157\n",
        "Best alpha:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.01 Training error: 2099.10386565 Testing error: 2382.3337407\n",
        "Best alpha:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.01 Training error: 2522.4734712 Testing error: 2313.94164336\n",
        "Best alpha:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.01 Training error: 1971.21340924 Testing error: 3610.00362915\n",
        "Best alpha:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.01 Training error: 1998.95415544 Testing error: 5975.26951435\n",
        "Best alpha:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.01 Training error: 1651.26571173 Testing error: 4943.57701263\n",
        "Best alpha:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.01 Training error: 1708.59225104 Testing error: 2366.44898132\n",
        "Best alpha:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.01 Training error: 1749.07954359 Testing error: 9274.66016976\n",
        "Best alpha:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.01 Training error: 2583.21388183 Testing error: 1839.110989\n",
        "Best alpha:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.01 Training error: 2393.42514962 Testing error: 6532.43500245\n",
        "Best alpha:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.01 Training error: 1771.87312912 Testing error: 2944.62802997\n"
       ]
      }
     ],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#generating boxplots for first dataset\n",
      "data = np.empty((75,20))\n",
      "for i in range(20):\n",
      "    tr_file = path1+'cereales-tr-{0}.npy'.format(i)\n",
      "    tr_data = np.load(tr_file)\n",
      "    data[:,i] = tr_data[:,-1]\n",
      "plt.boxplot(data)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAEACAYAAACuzv3DAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
       "text": [
        "<matplotlib.figure.Figure at 0x7f13682a2e50>"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#generating boxplots for second dataset\n",
      "data = np.empty((90,20))\n",
      "for i in range(20):\n",
      "    tr_file = path2+'credit-tr-{0}.npy'.format(i)\n",
      "    tr_data = np.load(tr_file)\n",
      "    data[:,i] = tr_data[:,-1]\n",
      "plt.boxplot(data)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAW8AAAEACAYAAAB8nvebAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
       "text": [
        "<matplotlib.figure.Figure at 0x7f1345e42b50>"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Parte 2"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#sigmoid function\n",
      "def sigmoid(z):\n",
      "    return 1./(1.+np.exp(-z))\n",
      "\n",
      "#overall cost function for logistic regression\n",
      "def J(X, y, beta):\n",
      "    h = sigmoid(np.dot(X,beta))\n",
      "    return 0.5*np.dot(h-y,h-y)\n",
      "\n",
      "#implementation of stochastic gradient descent \n",
      "def gd_stochastic(X, y, alpha=0.5, eps=1e-5, max_iter=1000000):\n",
      "    m,n = X.shape\n",
      "    beta = np.zeros(n)\n",
      "    J1 = J(X, y, beta) #loss at previous iteration\n",
      "    for i in range(max_iter):\n",
      "        J0 = J1\n",
      "        for i in range(m):\n",
      "            beta += alpha*(y[i] - sigmoid(np.dot(X[i],beta))*X[i]\n",
      "        J1 = J(X,y,beta)\n",
      "        if np.abs(J1-J0)/J0 < eps: break\n",
      "    return beta\n",
      "\n",
      "#Implementation of Newton-Raphson method for linear regression\n",
      "def nr_linear(X, y, eps=1e-5, max_iter=1000000):\n",
      "    m,n = X.shape\n",
      "    beta = np.zeros(n)\n",
      "    J1 = J(X,y,beta) #loss at previous iteration \n",
      "    Hess = np.dot(X.T,X) #Hessian matrix\n",
      "    for i in range(max_iter):\n",
      "        J0 = J1\n",
      "        h = np.dot(X,beta)\n",
      "        dJ = np.dot(X.T,h-y)\n",
      "        beta -= np.linalg.solve(Hess, dJ)\n",
      "        J1 = J(X,y,beta)\n",
      "        if np.abs(J1-J0)/J0 < eps: break\n",
      "    return beta\n",
      "\n",
      "#Implementation of Newton-Raphson method for logistic regression\n",
      "def nr_logistic(X, y, eps=1e-5, max_iter=1000000):\n",
      "    m,n = X.shape\n",
      "    beta = np.zeros(n)\n",
      "    J1 = J(X,y,beta) #loss at previous iteration\n",
      "    Hess \n",
      "    for i in range(max_iter):\n",
      "        h = sigmoid(np.dot(X,beta))\n",
      "        D = np.diag(h*(1-h))\n",
      "        Hess = np.dot(X.T, np.dot(W, X))\n",
      "        J0 = J1\n",
      "        h = np.dot(X,)\n",
      "    return"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}