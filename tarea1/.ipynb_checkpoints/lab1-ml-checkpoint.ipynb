{
 "metadata": {
  "name": "",
  "signature": "sha256:6e8b9518ff77d46f85c5acf9d61544f7b5f55dd867a5c9764ae78cdda2683f29"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Parte 1"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#notebook settings\n",
      "%matplotlib inline\n",
      "\n",
      "#import some useful libraries and utilities\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.cross_validation import KFold\n",
      "\n",
      "#setting some paths\n",
      "path1='./cereales/'\n",
      "path2='./credit/'\n",
      "\n",
      "#parameters to try\n",
      "params = np.linspace(0.002,0.01,5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#overall cost function for linear regresion\n",
      "def J(X, y, beta):\n",
      "    h = np.dot(X,beta)\n",
      "    return 0.5*np.sum((h-y)**2)\n",
      "\n",
      "#Implementation of batch gradient descent\n",
      "def gd_batch(X, y, alpha=0.5, eps=10e-5):\n",
      "    m,n = X.shape\n",
      "    beta = np.zeros(n)\n",
      "    J1 = J(X,y,beta) #loss at previous iteration\n",
      "    while(True):\n",
      "        J0 = J1\n",
      "        h = np.dot(X,beta)\n",
      "        dJ = np.dot(X.T,h-y)\n",
      "        beta -= alpha*dJ\n",
      "        J1 = J(X,y,beta)\n",
      "        if np.abs(J1-J0)/J0 < eps: break\n",
      "    return beta\n",
      "\n",
      "#Implementation of online gradient descent\n",
      "def gd_online(X, y, alpha=0.5, eps=10e-5):\n",
      "    m,n = X.shape\n",
      "    beta = np.zeros(n)\n",
      "    J1 = J(X,y,beta) #loss at previous iteration\n",
      "    while(True):\n",
      "        J0 = J1\n",
      "        for i in range(m):\n",
      "            beta -= alpha*(np.dot(X[i],beta)-y[i])*X[i]\n",
      "        J1 = J(X,y,beta)\n",
      "        if np.abs(J1-J0)/J0 < eps: break\n",
      "    return beta\n",
      "\n",
      "#Implementation of Newton-Raphson method for linear regression\n",
      "def nr_linear(X, y, eps=10e-5):\n",
      "    m,n = X.shape\n",
      "    beta = np.zeros(n)\n",
      "    J1 = J(X,y,beta) #loss at previous iteration \n",
      "    Hess = np.dot(X.T,X) #Hessian matrix\n",
      "    while(True):\n",
      "        J0 = J1\n",
      "        h = np.dot(X,beta)\n",
      "        dJ = np.dot(X.T,h-y)\n",
      "        beta -= np.linalg.solve(Hess, dJ)\n",
      "        J1 = J(X,y,beta)\n",
      "        if np.abs(J1-J0)/J0 < eps: break\n",
      "    return beta\n",
      "\n",
      "#Implementation of weighted gradient descent\n",
      "def gd_weighted(X, y):\n",
      "    return\n",
      "\n",
      "#Implementation of Newton-Raphson method for logistic regression\n",
      "def nr_logistic(X, y, n_iter=50):\n",
      "    return\n",
      "\n",
      "def rescale(M,a,b):\n",
      "    \"\"\" Rescale features of M to [a,b] range \"\"\"\n",
      "    #max and min vectors\n",
      "    maxv = np.max(M, axis=0)\n",
      "    minv = np.min(M, axis=0)\n",
      "    return (b-a)*M/(maxv-minv) + (a*maxv-b*minv)/(maxv-minv)\n",
      "\n",
      "def normalize(M):\n",
      "    #mean and standard deviation vectors\n",
      "    meanv = np.mean(M, axis=0)\n",
      "    stdv = np.std(M, axis=0)\n",
      "    return (M-meanv)/stdv"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#working array\n",
      "wa = np.load(path1+'cereales-tr-0.npy')\n",
      "rescaled_wa = rescale(wa,0.,1.)\n",
      "normalized_wa = normalize(wa)\n",
      "X = rescaled_wa[:,:-1]\n",
      "y = rescaled_wa[:,-1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "beta1 = gd_batch(X, y, alpha=0.005)\n",
      "print 'beta:',beta1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "beta: [-0.28498565  0.23926439 -0.18026069 -0.19498143  0.66352834  0.2947549\n",
        " -0.16185107 -0.12150056 -0.18963035  0.11197557  0.32566004  0.52589985]\n"
       ]
      }
     ],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "beta2 = gd_online(X, y, alpha=0.005)\n",
      "print 'beta:',beta2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "beta: [-0.284187    0.24098783 -0.18022872 -0.19401827  0.66518596  0.29444591\n",
        " -0.16144881 -0.12269884 -0.18915525  0.11251132  0.32425563  0.52679119]\n"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "beta3 = nr_linear(X,y)\n",
      "print 'beta:',beta3"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "beta: [-0.28477401  0.23382422 -0.1711402  -0.19611558  0.766077    0.28185174\n",
        " -0.17850135 -0.22100888 -0.19930546  0.11996579  0.36355978  0.52893807]\n"
       ]
      }
     ],
     "prompt_number": 36
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\" Find the best learning parameter for algorithm, between \n",
      "parameters in params using k-fold cross validation \"\"\"\n",
      "def find_best(X, y, algorithm, params, mode=None):\n",
      "    #creating kfold\n",
      "    m,n = X.shape\n",
      "    kf = KFold(m, n_folds=5)\n",
      "    tr_cost = list()\n",
      "    ts_cost = list()\n",
      "    \n",
      "    for param in params:\n",
      "        mean_tr_cost = 0\n",
      "        mean_ts_cost = 0\n",
      "        for tr_index,ts_index in kf:\n",
      "            X_train, X_test = X[tr_index], X[ts_index]\n",
      "            y_train, y_test = y[tr_index], y[ts_index]\n",
      "            beta = algorithm(X_train, y_train, alpha=param)\n",
      "            mean_tr_cost += J(X_train, y_train, beta)\n",
      "            mean_ts_cost += J(X_test, y_test, beta)\n",
      "        tr_cost.append(mean_tr_cost/5)\n",
      "        ts_cost.append(mean_ts_cost/5)\n",
      "    if mode=='verbose':\n",
      "        #print some info\n",
      "        print 'Mean training errors for each alpha:'\n",
      "        print tr_cost\n",
      "        print 'Mean testing errors for each alpha:'\n",
      "        print ts_cost\n",
      "    return params[np.argmin(np.array(ts_cost))]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "find_best(X, y, gd_batch, params, mode='verbose')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Mean training errors for each alpha:\n",
        "[0.14346357539067406, 0.13749300306475862, 0.13554424716673849, 0.13457225314625129, 0.13399356971691684]\n",
        "Mean testing errors for each alpha:\n",
        "[0.054066821646794251, 0.052022098345865055, 0.051443667229546394, 0.051186865059925687, 0.051054242620090715]\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 41,
       "text": [
        "0.01"
       ]
      }
     ],
     "prompt_number": 41
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for i in range(20):\n",
      "    tr_file = path1+'cereales-tr-{0}.npy'.format(i)\n",
      "    ts_file = path1+'cereales-ts-{0}.npy'.format(i)\n",
      "    tr_data = np.load(tr_file)\n",
      "    ts_data = np.load(ts_file)\n",
      "    X_tr = rescale(tr_data[:,:-1], 0., 1.)\n",
      "    y_tr = tr_data[:,-1]\n",
      "    X_ts = rescale(ts_data[:,:-1], 0., 1.)\n",
      "    y_ts = ts_data[:,-1]\n",
      "    alpha = find_best(X_tr, y_tr, gd_batch, params)\n",
      "    beta = gd_batch(X_tr, y_tr, alpha)\n",
      "    print 'Best alpha:',alpha,'Training error:',J(X_tr,y_tr,beta),'Testing error:',J(X_ts,y_ts,beta)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Best alpha: 0.01 Training error: 2071.04108657 Testing error: 2484.76100314\n",
        "Best alpha:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.01 Training error: 2000.6253735 Testing error: 2295.26584449\n",
        "Best alpha:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.01 Training error: 2781.38270249 Testing error: 3900.70227886\n",
        "Best alpha:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.01 Training error: 2675.56924202 Testing error: 3905.75511688\n",
        "Best alpha:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.01 Training error: 1834.00171907 Testing error: 1804.39319415\n",
        "Best alpha:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.004 Training error: 1616.7222372 Testing error: 2386.38191037\n",
        "Best alpha:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.01 Training error: 1974.21009071 Testing error: 5041.79565741\n",
        "Best alpha:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.008 Training error: 1898.07237923 Testing error: 1869.98188998\n",
        "Best alpha:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.01 Training error: 2154.11535571 Testing error: 2100.74411243\n",
        "Best alpha:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.01 Training error: 1992.80711704 Testing error: 8974.27027157\n",
        "Best alpha:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.01 Training error: 2099.10386565 Testing error: 2382.3337407\n",
        "Best alpha:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.01 Training error: 2522.4734712 Testing error: 2313.94164336\n",
        "Best alpha:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.01 Training error: 1971.21340924 Testing error: 3610.00362915\n",
        "Best alpha:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.01 Training error: 1998.95415544 Testing error: 5975.26951435\n",
        "Best alpha:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.01 Training error: 1651.26571173 Testing error: 4943.57701263\n",
        "Best alpha:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.01 Training error: 1708.59225104 Testing error: 2366.44898132\n",
        "Best alpha:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.01 Training error: 1749.07954359 Testing error: 9274.66016976\n",
        "Best alpha:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.01 Training error: 2583.21388183 Testing error: 1839.110989\n",
        "Best alpha:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.01 Training error: 2393.42514962 Testing error: 6532.43500245\n",
        "Best alpha:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.01 Training error: 1771.87312912 Testing error: 2944.62802997\n"
       ]
      }
     ],
     "prompt_number": 39
    }
   ],
   "metadata": {}
  }
 ]
}